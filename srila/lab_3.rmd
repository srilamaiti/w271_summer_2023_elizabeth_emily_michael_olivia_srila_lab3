---
title: 'Lab 3: Panel Models'
subtitle: 'US Traffic Fatalities: 1980 - 2004'
output: 
  bookdown::pdf_document2: default
---

```{r load packages, echo=FALSE, message=FALSE}
library(GGally)
library(fredr)
library(corrplot)
library(car)
library(ggrepel)
library(tibble)
library(lme4)
library(gridExtra)
library(reshape2)
library(mgcv)
library(plm)
library(lmtest)
library(aTSA)
library(urca)
library(tsibble)
library(dplyr)
library(fabletools)
library(tidyverse)
library(magrittr)
library(patchwork)
library(scales)
library(plyr)
library(tidyr)
library(ggplot2)
library(ggthemes)
library(lubridate)
library(forecast)
library(sandwich)
library(tseries)
library(vars)
library(jsonlite)
library(fable)
library(gtrendsR) 
library(zoo)
library(feasts)
library(thematic)
library(ggfortify)
library(fpp3)
require(knitr)
library(stargazer)
knitr::opts_chunk$set(tidy = TRUE, tidy.opts = list(comment = FALSE))
knitr::opts_chunk$set(comment = " ")
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```


# U.S. traffic fatalities: 1980-2004

In this lab, we are asking you to answer the following **causal** question: 

> **"Do changes in traffic laws affect traffic fatalities?"**  

To answer this question, please complete the tasks specified below using the data 
provided in `data/driving.Rdata`. This data includes 25 years of data that cover 
changes in various state drunk driving, seat belt, and speed limit laws. 

Specifically, this data set contains data for the 48 continental U.S. states from 
1980 through 2004. Various driving laws are indicated in the data set, such as the 
alcohol level at which drivers are considered legally intoxicated. There are also 
indicators for “per se” laws—where licenses can be revoked without a trial—and seat 
belt laws. A few economics and demographic variables are also included. The 
description of the each of the variables in the dataset is also provided in 
the dataset. 

```{r load data, echo = TRUE}
load(file="/Users/srilamaiti/Downloads/data/driving.RData")
glimpse(data)
desc
```


# (30 points, total) Build and Describe the Data 

1. (5 points) Load the data and produce useful features.
    - Produce a new variable, called `speed_limit` that re-encodes the data that 
    is in `sl55`, `sl65`, `sl70`, `sl75`, and `slnone`; 
    - Produce a new variable, called `year_of_observation` that re-encodes the 
    data that is in `d80`, `d81`, ... , `d04`. 
    - Produce a new variable for each of the other variables that are one-hot 
    encoded (i.e. `bac*` variable series). 
    - Rename these variables to sensible names that are legible to a reader of 
    your analysis. For example, the dependent variable as provided is called, 
    `totfatrte`. Pick something more sensible, like, `total_fatalities_rate`. 
    There are few enough of these variables to change, that you should change 
    them for all the variables in the data. (You will thank yourself later.)

# Feature Engineering

We have added state abriviation in the data along with DC.
```{r, adding state abriviation}
# Adding state abbriviation
state_avbbriv <- data.frame("index"=1:51,"abbr"=sort(c(state.abb,"DC")))
data <- merge(data, state_avbbriv, by.x = "state", by.y = "index")
```

We then added year of observation in the data set.
```{r, adding yeer of observation}
# year_of_observation
data <- data %>%
  mutate(year_of_observation = factor(year))
```

Next, we have manipulated speed limit and seat belt related columns:-
(sl55/sl65/sl70/s75/slone). Although, we have expected the data to be binary across 
speed limit columns, we have noticed fractional distributions and we devised a 
rule where max value will be considered from these percentage values. We also
created another binary column speed_limit based on speed_limit. If the value
in speed_limit is equal or more than 70, the column will be set to 1, otherwise 0.
```{r, adding speed limit and seat belt feature}
# speed_limit
data <- data %>%
  mutate(speed_limit = ifelse(sl55 >= 0.5, 55,
                       ifelse(sl65 >= 0.5, 65,
                       ifelse(sl70 >= 0.5, 70,
                       ifelse(sl75 >= 0.5, 75,
                       ifelse(slnone >= 0.5, 'none', 0)
                       ))))) 

data = data %>% mutate(
    seatbelt = factor(seatbelt), # 'seatbelt' categorizes primary or secondary
    speed_limit_70plus = ifelse(speed_limit == 55 | speed_limit == 65, 0, 1)
  )
```

```{r, blood alcohol limit}
# Rounding bac08 and bac10
data <- data %>% 
  mutate(
    bac08_processed = round(bac08),
    bac10_processed = round(bac10)
  )
# blood alcohol limit
data <- data %>% mutate(blood_alcohol_limit_10 = ifelse(bac10 >= 0.5, 1, 0),
                        blood_alcohol_limit_08 = ifelse(bac08 >= 0.5, 1, 0)
                       ) %>% 
                mutate(bac = ifelse(blood_alcohol_limit_10==1, '10', 
                             ifelse(blood_alcohol_limit_08==1, '8', 'none')))

```

```{r, renaming to meaningful columns}
# rename the variables to sensible names
data <- data %>%
  dplyr::rename(
    "total_fatalities_rate"                      = "totfatrte",
    "minimum_drinking_age"                       = "minage",
    "zero_tolerance_law"                         = "zerotol",
    "state_population"                           = "statepop",
    "graduated_drivers_license_law"              = "gdl",
    "per_se_laws"                                = "perse",
    "total_traffic_fatalities"                   = "totfat",
    "total_nighttime_fatalities"                 = "nghtfat",
    "total_weekend_fatalities"                   = "wkndfat",
    "total_fatalities_per_100_million_miles"     = "totfatpvm",
    "nighttime_fatalities_per_100_million_miles" = "nghtfatpvm",
    "weekend_fatalities_per_100_million_miles"   = "wkndfatpvm",
    "nighttime_fatalities_rate"                  = "nghtfatrte",
    "weekend_fatalities_rate"                    = "wkndfatrte",
    "vehicle_miles"                              = "vehicmiles",
    "unemployment_rate"                          = "unem",
    "pct_population_14_to_24"                    = "perc14_24",
    "vehicle_miles_per_capita"                   = "vehicmilespc",
    "primary_seatbelt_law"                       = "sbprim",
    "secondary_seatbelt_law"                     = "sbsecon"
  ) 

colnames(data)
```

2. (5 points) Provide a description of the basic structure of the dataset. What is 
this data? How, where, and when is it collected? Is the data generated through a 
survey or some other method? Is the data that is presented a sample from the population, 
or is it a *census* that represents the entire population? Minimally, this should include:
    - How is the our dependent variable of interest `total_fatalities_rate` defined? 
    
> The data is for state-year-level total traffic fatalities information. It includes 
state law information that might be an indicator for traffic fatalities, like the 
speed limit, blood alcohol legal limit, seat belt laws, etc. The data was compiled 
by Donald G Freedman for the paper "Drunk living legislation and traffic fatalities:
New evidence on BAC 08 laws" - Contemporary Economic Policy 2007. In the paper 
it is noted that "Fatality data are from the Fatality Analysis Reporting System (FARS) 
compiled by NHTSA. Data on traffic legislation for the years 1982—1999 were provided 
by Thomas Dee. Earlier data on legislation were taken from Zador et al. (1989) 
and later data on legislation from the National Center for Statistics and Analysis 
at the NHTSA Web site at http://www-nrd.nhtsa.dot.gov/departments/nrd-30/ncsa/. 
Data on graduated drivers’ licenses are taken from Dee, Grabowski, and Morrisey 
(2005). State unemployment rates are from Dee and the Bureau of Labor Statistics; 
age data are from the Bureau of the Census".The outcome of interest, 
`total_fatalities_rate` is defined as the number of fatalities per 100,000 people.

3. (20 points) Conduct a very thorough EDA, which should include both graphical 
and tabular techniques, on the dataset, including both the dependent variable 
`total_fatalities_rate` and the potential explanatory variables. Minimally, this 
should include: 
    - How is the our dependent variable of interest `total_fatalities_rate` defined? 
    - What is the average of `total_fatalities_rate` in each of the years in the 
    time period covered in this dataset? 

As with every EDA this semester, the goal of this EDA is not to document your own 
process of discovery -- save that for an exploration notebook -- but instead it is 
to bring a reader that is new to the data to a full understanding of the important 
features of your data as quickly as possible. In order to do this, your EDA should 
include a detailed, orderly narrative description of what you want your reader to 
know. Do not include any output -- tables, plots, or statistics -- that you do not 
intend to write about.

```{r, data-distribution check}
data$zero_tolerance_law %>% table(useNA = "ifany") %>% as.data.frame()
data$graduated_drivers_license_law %>% table(useNA = "ifany") %>% as.data.frame()
data$per_se_laws %>% table(useNA = "ifany") %>% as.data.frame()
data$minimum_drinking_age %>% table(useNA = "ifany") %>% as.data.frame()
```

```{r, adding binary variables}

# Binary variable zero_tolerance_law_refined based on zero_tolerance_law
data <- data %>% 
     mutate(zero_tolerance_law_refined = ifelse(zero_tolerance_law < 0.5, 0, 1))

# Binary variable graduated_drivers_license_law_refined based on graduated_drivers_license_law
data <- data %>% 
     mutate(graduated_drivers_license_law_refined = ifelse(graduated_drivers_license_law < 0.5, 0, 1))

# Binary variable per_se_laws_refined based on per_se_laws
data <- data %>% 
     mutate(per_se_laws_refined = ifelse(per_se_laws < 0.5, 0, 1))

# Rounding min drinking age
data <- data %>%
  mutate(
    minimum_drinking_age = round(minimum_drinking_age, 0)
  )
```
```{r, box plot yearly fatality rate}
boxplot(total_fatalities_rate ~ year_of_observation, data = data)
```
```{r, box plot state fatality rate}
boxplot(total_fatalities_rate ~ state, data = data)
```
```{r, state wise avg fatality rate plot}
data %>% ggplot(aes(reorder(state,desc(total_fatalities_rate)),total_fatalities_rate,
             fill=state)) +
  geom_boxplot(alpha=0.4) +
  # theme_economist_white(gray_bg=F) +
  theme(legend.position="none",axis.text.y=element_text(size=6)) +
  scale_y_continuous() +
  xlab("State") +
  ylab("Total_Fatalities_Rate")
```
```{r}
data %>%
    ggplot(aes(year,total_fatalities_rate,color=state)) +
    geom_point(alpha=0.4) +
    geom_smooth(method="lm") +
    facet_wrap(~state,scales="free_y") +
    theme_economist_white(gray_bg=F) +
    theme(legend.position="none",axis.text.x=element_text(angle=45,hjust=1,vjust=1,size=6),
          axis.text.y=element_text(size=6)) +
    theme(strip.text=element_text(size=4)) +
    scale_y_continuous() +
    xlab("State") +
    ylab("Total_Fatalities_Rate")
```
```{r}
p1<- data %>%
  filter(state <= 12 ) %>%
  ggplot(aes(x = year_of_observation, y = total_fatalities_rate)) +
  geom_line(aes(color = state)) +
  labs(x = "Year",  y = "Total Fatality rate") +
  geom_label_repel(data = filter(data, state <= 12  & year == 1984),
                   aes(label = state), nudge_x = .75,na.rm = TRUE) +
  theme(legend.position = "none")

p2<- data %>%
   filter(state > 12 & state <= 24 ) %>%
  ggplot(aes(x = year_of_observation, y = total_fatalities_rate)) +
  geom_line(aes(color = state)) +
  labs(x = "Year",  y = "Fatality rate")+
  geom_label_repel(data = filter(data, state > 12 & as.integer(state) <= 24  & year == 1984),
                   aes(label = state), nudge_x = .75,na.rm = TRUE) +
  theme(legend.position = "none")

p3<- data %>%
  filter(as.integer(state) > 24 &as.integer(state) <= 36 ) %>%
  ggplot(aes(x = year_of_observation, y = total_fatalities_rate)) +
  geom_line(aes(color = state)) +
  labs(x = "Year",  y = "Fatality rate")+
  geom_label_repel(data = filter(data, state > 24 &as.integer(state) <= 36 & year == 1984),
                   aes(label = state), nudge_x = .75,na.rm = TRUE) +
  theme(legend.position = "none")

p4<- data %>%
  filter(as.integer(state) > 36 ) %>%
  ggplot(aes(x = year_of_observation, y = total_fatalities_rate)) +
  geom_line(aes(color = state)) +
  labs(x = "Year",  y = "Fatality rate") +
  geom_label_repel(data = filter(data, state > 36 & year == 1984),aes(label = state),
                   nudge_x = .75,na.rm = TRUE) +
  theme(legend.position = "none")

grid.arrange(p1,p2,p3, p4, nrow = 2, ncol = 2)
```
```{r, histogram plot}
plot_hist = geom_histogram(aes(y=..count..), bins = 40, color = "black", 
                           fill = "blue")
plot_theme = theme(plot.title = element_text(face = "bold", lineheight = 1),
                   axis.title.x=element_blank(), axis.text.y = element_blank())
h1 = ggplot(data, aes(x = total_traffic_fatalities)) + 
               scale_x_continuous(name = "Total Fatal.") +
               plot_hist + plot_theme

h2 = ggplot(data, aes(x = log(total_traffic_fatalities))) + 
               scale_x_continuous(name = "Log(Total Fatal.)") +
               plot_hist + plot_theme + plot_theme

h3 = ggplot(data, aes(x = total_weekend_fatalities)) +
               scale_x_continuous(name = "Wknd. Fatal.") +
               plot_hist + plot_theme

h4 = ggplot(data, aes(x = log(total_weekend_fatalities))) +
               scale_x_continuous(name = "Log(Wknd. Fatal.)") +
               plot_hist + plot_theme + plot_theme

h5 = ggplot(data, aes(x = total_nighttime_fatalities)) +
               scale_x_continuous(name = "Night Fatal.") +
               plot_hist + plot_theme

h6 = ggplot(data, aes(x = log(total_nighttime_fatalities))) +
               scale_x_continuous(name = "Log(Night Fatal.)") +
               plot_hist + plot_theme + plot_theme

h7 = ggplot(data, aes(x = total_fatalities_per_100_million_miles)) +
               scale_x_continuous(name = "Total per Mile Rate") +
               plot_hist + plot_theme

h8 = ggplot(data, aes(x = log(total_fatalities_per_100_million_miles))) +
               scale_x_continuous(name = "Log(Total per Mile Rate)") +
               plot_hist + plot_theme + plot_theme

h9 = ggplot(data, aes(x = weekend_fatalities_per_100_million_miles)) +
               scale_x_continuous(name = "Wknd per Mile Rate") +
               plot_hist + plot_theme

h10 = ggplot(data, aes(x = log(weekend_fatalities_per_100_million_miles))) +
               scale_x_continuous(name = "Log(Wknd per Mile Rate)") +
               plot_hist + plot_theme + plot_theme

h11 = ggplot(data, aes(x = nighttime_fatalities_per_100_million_miles)) +
               scale_x_continuous(name = "Night per Mile Rate") +
               plot_hist + plot_theme

h12 = ggplot(data, aes(x = log(nighttime_fatalities_per_100_million_miles))) +
               scale_x_continuous(name = "Log(Night per Mile Rate)") +
               plot_hist + plot_theme + plot_theme

h13 = ggplot(data, aes(x = total_fatalities_rate)) +
               scale_x_continuous(name = "Total per Cap Rate") +
               plot_hist + plot_theme

h14 = ggplot(data, aes(x = log(total_fatalities_rate))) +
               scale_x_continuous(name = "Log(Total per Cap Rate)") +
               plot_hist + plot_theme + plot_theme

h15 = ggplot(data, aes(x = weekend_fatalities_rate)) +
               scale_x_continuous(name = "Wknd per Cap Rate") +
               plot_hist + plot_theme

h16 = ggplot(data, aes(x = log(weekend_fatalities_rate))) +
               scale_x_continuous(name = "Log(Wknd per Cap Rate)") +
               plot_hist + plot_theme + plot_theme

h17 = ggplot(data, aes(x = nighttime_fatalities_rate)) +
               scale_x_continuous(name = "Night per Cap Rate") +
               plot_hist + plot_theme

h18 = ggplot(data, aes(x = log(nighttime_fatalities_rate))) +
               scale_x_continuous(name = "Log(Night per Cap Rate)") +
               plot_hist + plot_theme

grid.arrange(h1,h2,h3,h4,h5,h6,h7,h8,h9,h10,h11,h12,h13,h14,h15,h16,h17,h18,
             nrow = 5, ncol = 4,
             top=quote("Fatalities, Log(Fatalities) Distribution"))
```
```{r, state box plot}
box_st_1 <- data %>%
  group_by(state) %>%
  ggplot(aes(x = reorder(state,total_traffic_fatalities), 
             y = total_traffic_fatalities)) +
  geom_boxplot() +
  labs(x = "States",  y = "Total Fatality rate")

box_st_2 <- data %>%
  group_by(state) %>%
  ggplot(aes(x = reorder(state,total_weekend_fatalities), 
             y = total_weekend_fatalities)) +
  geom_boxplot() +
  labs(x = "States",  y = "Wknd. Fatality rate")

box_st_3 <- data %>%
  group_by(state) %>%
  ggplot(aes(x = reorder(state,total_nighttime_fatalities), 
             y = total_nighttime_fatalities)) +
  geom_boxplot() +
  labs(x = "States",  y = "Night Fatality rate")

box_st_4 <- data %>%
  group_by(state) %>%
  ggplot(aes(x = reorder(state,total_fatalities_per_100_million_miles), 
             y = total_fatalities_per_100_million_miles)) +
  geom_boxplot() +
  labs(x = "States",  y = "Total per Mile Rate")

box_st_5 <- data %>%
  group_by(state) %>%
  ggplot(aes(x = reorder(state,weekend_fatalities_per_100_million_miles), 
             y = weekend_fatalities_per_100_million_miles)) +
  geom_boxplot() +
  labs(x = "States",  y = "Wknd per Mile Rate")

box_st_6 <- data %>%
  group_by(state) %>%
  ggplot(aes(x = reorder(state,nighttime_fatalities_per_100_million_miles), 
             y = nighttime_fatalities_per_100_million_miles)) +
  geom_boxplot() +
  labs(x = "States",  y = "Night per Mile Rate")

box_st_7 <- data %>%
  group_by(state) %>%
  ggplot(aes(x = reorder(state,total_fatalities_rate), 
             y = total_fatalities_rate)) +
  geom_boxplot() +
  labs(x = "States",  y = "Total per Cap. Rate")

box_st_8 <- data %>%
  group_by(state) %>%
  ggplot(aes(x = reorder(state,weekend_fatalities_rate), 
             y = weekend_fatalities_rate)) +
  geom_boxplot() +
  labs(x = "States",  y = "Wknd. per Cap. Rate")

box_st_9 <- data %>%
  group_by(state) %>%
  ggplot(aes(x = reorder(state,nighttime_fatalities_rate), 
             y = nighttime_fatalities_rate)) +
  geom_boxplot() +
  labs(x = "States",  y = "Night per Cap Rate")

grid.arrange(box_st_1,box_st_2,box_st_3,box_st_4,box_st_5,box_st_6,box_st_7,
             box_st_8,box_st_9,nrow = 3, ncol = 3,
             top=quote("Boxplot of Fatalities by State 1980 - 2004"))
```
```{r, year box plot}
box_yr_1 <- data %>%
  group_by(year) %>%
  ggplot(aes(x = year_of_observation, 
             y = total_traffic_fatalities)) +
  geom_boxplot() +
  labs(x = "years",  y = "Total Fatality rate")

box_yr_2 <- data %>%
  group_by(year) %>%
  ggplot(aes(x = year_of_observation, 
             y = total_weekend_fatalities)) +
  geom_boxplot() +
  labs(x = "years",  y = "Wknd. Fatality rate")

box_yr_3 <- data %>%
  group_by(year) %>%
  ggplot(aes(x = year_of_observation, 
             y = total_nighttime_fatalities)) +
  geom_boxplot() +
  labs(x = "years",  y = "Night Fatality rate")

box_yr_4 <- data %>%
  group_by(year) %>%
  ggplot(aes(x = year_of_observation, 
             y = total_fatalities_per_100_million_miles)) +
  geom_boxplot() +
  labs(x = "years",  y = "Total per Mile Rate")

box_yr_5 <- data %>%
  group_by(year) %>%
  ggplot(aes(x = year_of_observation, 
             y = weekend_fatalities_per_100_million_miles)) +
  geom_boxplot() +
  labs(x = "years",  y = "Wknd per Mile Rate")

box_yr_6 <- data %>%
  group_by(year) %>%
  ggplot(aes(x = year_of_observation, 
             y = nighttime_fatalities_per_100_million_miles)) +
  geom_boxplot() +
  labs(x = "years",  y = "Night per Mile Rate")

box_yr_7 <- data %>%
  group_by(year) %>%
  ggplot(aes(x = year_of_observation, 
             y = total_fatalities_rate)) +
  geom_boxplot() +
  labs(x = "years",  y = "Total per Cap. Rate")

box_yr_8 <- data %>%
  group_by(year) %>%
  ggplot(aes(x = year_of_observation, 
             y = weekend_fatalities_rate)) +
  geom_boxplot() +
  labs(x = "years",  y = "Wknd. per Cap. Rate")

box_yr_9 <- data %>%
  group_by(year) %>%
  ggplot(aes(x = year_of_observation, 
             y = nighttime_fatalities_rate)) +
  geom_boxplot() +
  labs(x = "years",  y = "Night per Cap Rate")

grid.arrange(box_yr_1,box_yr_2,box_yr_3,box_yr_4,box_yr_5,box_yr_6,box_yr_7,
             box_yr_8,box_yr_9, nrow = 3, ncol = 3,
             top=quote("Boxplot of Fatalities over years 1980 - 2004"))
```
```{r, histogram of explanatory variables}
ex_h1 = ggplot(data, aes(x = minimum_drinking_age)) + 
               scale_x_continuous(name = "Minimum Age") +
               plot_hist + plot_theme

ex_h2 = ggplot(data, aes(x = log(minimum_drinking_age))) + 
               scale_x_continuous(name = "Log(Minimum Age)") +
               plot_hist + plot_theme

ex_h3 = ggplot(data, aes(x = state_population)) + 
               scale_x_continuous(name = "State Population") +
               plot_hist + plot_theme

ex_h4 = ggplot(data, aes(x = log(state_population))) + 
               scale_x_continuous(name = "Log(State Population)") +
               plot_hist + plot_theme

ex_h5 = ggplot(data, aes(x = vehicle_miles)) + 
               scale_x_continuous(name = "Miles Trvld") +
               plot_hist + plot_theme

ex_h6 = ggplot(data, aes(x = log(vehicle_miles))) + 
               scale_x_continuous(name = "Log(Miles Trvld)") +
               plot_hist + plot_theme

ex_h7 = ggplot(data, aes(x = unemployment_rate)) + 
               scale_x_continuous(name = "Unemp. Rate") +
               plot_hist + plot_theme

ex_h8 = ggplot(data, aes(x = log(unemployment_rate))) + 
               scale_x_continuous(name = "Log(Unemp. Rate)") +
               plot_hist + plot_theme

ex_h9 = ggplot(data, aes(x = pct_population_14_to_24)) + 
               scale_x_continuous(name = "% Pop 14-24") +
               plot_hist + plot_theme

ex_h10 = ggplot(data, aes(x = log(pct_population_14_to_24))) + 
               scale_x_continuous(name = "Log(% Pop 14-24)") +
               plot_hist + plot_theme

ex_h11 = ggplot(data, aes(x = vehicle_miles_per_capita)) + 
               scale_x_continuous(name = "Miles Driven") +
               plot_hist + plot_theme

ex_h12 = ggplot(data, aes(x = log(vehicle_miles_per_capita))) + 
               scale_x_continuous(name = "log(Miles Driven)") +
               plot_hist + plot_theme

ex_h13 = ggplot(data, aes(x = as.numeric(speed_limit))) + 
               scale_x_continuous(name = "Speed Limit") +
               plot_hist + plot_theme

ex_h14 = ggplot(data, aes(x = log(as.numeric(speed_limit)))) + 
               scale_x_continuous(name = "Log(Speed Limit)") +
               plot_hist + plot_theme

grid.arrange(ex_h1,ex_h2,ex_h3,ex_h4,ex_h5,ex_h6,ex_h7,
             ex_h8,ex_h9,ex_h10,ex_h11,ex_h12,ex_h13,ex_h14,
             nrow = 7, ncol = 2,
             top=quote("Distributions of Explanatory Variables and Logs"))
```

```{r, state level avg}
state.mean <- aggregate(data[, c("total_fatalities_rate", 
                               "state_population",
                               "unemployment_rate",
                               "pct_population_14_to_24",
                               "vehicle_miles",
                               "vehicle_miles_per_capita")], 
                         data["state"], FUN = mean)
ggpairs(state.mean, size = 5, cardinality_threshold = 60) + 
  ggtitle(label = "Year Averages over State")
```
```{r, year level avg}
year.mean <- aggregate(data[, c("total_fatalities_rate", 
                                    "state_population",
                                    "unemployment_rate",
                                    "pct_population_14_to_24",
                                    "vehicle_miles",
                                    "vehicle_miles_per_capita")], 
                         data["year_of_observation"], FUN = mean)
ggpairs(year.mean, size = 5, cardinality_threshold=60) + 
  ggtitle(label = "State Averages over time (years)")
```
```{r, fatality rate vs year}
yearly.law.mean <- aggregate(data[,c("total_fatalities_rate", 
                                        "minimum_drinking_age", 
                                        "zero_tolerance_law",
                                        "speed_limit_70plus",
                                        "graduated_drivers_license_law",
                                        "per_se_laws",
                                        "blood_alcohol_limit",
                                        "primary_seatbelt_law",
                                        "secondary_seatbelt_law")], 
                              data["year_of_observation"], FUN = mean)
ggpairs(yearly.law.mean, size = 5, cardinality_threshold=100) + 
  ggtitle(label = "State Law Proportions over time (years)")
```
```{r}
state.law.mean <- aggregate(data[,c("total_fatalities_rate", 
                                        "minimum_drinking_age", 
                                        "zero_tolerance_law",
                                        "speed_limit_70plus",
                                        "graduated_drivers_license_law",
                                        "per_se_laws",
                                        "blood_alcohol_limit",
                                        "primary_seatbelt_law",
                                        "secondary_seatbelt_law")], 
                              data["state"], FUN = mean)
ggpairs(state.law.mean, size = 5, cardinality_threshold=100) + 
  ggtitle(label = "State Law Time Proportions vs State")
```

# (15 points) Preliminary Model

Estimate a linear regression model of *totfatrte* on a set of dummy variables for 
the years 1981 through 2004 and interpret what you observe. In this section, you 
should address the following tasks: 

- Why is fitting a linear model a sensible starting place? 
- What does this model explain, and what do you find in this model? 
- Did driving become safer over this period? Please provide a detailed explanation.
- What, if any, are the limitation of this model. In answering this, please consider 
**at least**: 
    - Are the parameter estimates reliable, unbiased estimates of the truth? Or, 
    are they biased due to the way that the data is structured?
    - Are the uncertainty estimate reliable, unbiased estimates of sampling based 
    variability? Or, are they biased due to the way that the data is structured? 

```{r, base model}
model.prelim <- plm(log(total_fatalities_rate) ~ year_of_observation, 
                    data = data, 
                    index = c("state", "year"),
                    effect = "individual", 
                    model = "pooling")
summary(model.prelim)
coeftest(model.prelim, vcov. = vcovHC, type = "HC1")
```
Starting with a linear model provides an easy and intuitive way to explain the 
overall trend over the years via the dummy variables coefficients. We have used
log of the total fatalities rate as the distribution showed skewness.

This model explains the log of fatality rate over different years compared to the 
reference year 1980. All coefficients are statistically significant. The model 
explains that  model that the fatality rate goes down compared to the reference 
year 1980. In the period of time, the fatality ratekeeps decreasing. 
Ex. Fatality rate in 1990 `r round(10 ^ coef(model.prelim)['year_of_observation1990'], 2)`
is more than the fatality rate in 2000 `r round(10 ^ coef(model.prelim)['year_of_observation2000'], 2)`.

Here we are ignoring unobserved Heterogeneity and the group structure by 
considering each entry as a separate observation. Because of that, residuals 
generally correlate across time and have heteroskedasticity across and/or within 
groups. Heteroscedastic residuals are a violation of the OLS Homoscedasticity 
assumption, which will make it difficult to trust the standard error. As a result, 
the confidence interval can not be trusted as it can be too wide or narrow. 
Also, the independence assumption (no autocorrelation) is violated since we did 
not accommodate the lag/trend component, which makes the OLS estimates to be unreliable; 
in other words, our OLS estimator is not the Best Linear Unbiased Estimator.

# (15 points) Expanded Model 

Expand the **Preliminary Model** by adding variables related to the following 
concepts: 

- Blood alcohol levels 
- Per se laws
- Primary seat belt laws (Note that if a law was enacted sometime within a year the 
fraction of the year is recorded in place of the zero-one indicator.)
- Secondary seat belt laws 
- Speed limits faster than 70 
- Graduated drivers licenses 
- Percent of the population between 14 and 24 years old
- Unemployment rate
- Vehicle miles driven per capita. 

```{r Expanded Model}
data$bac <- relevel(factor(data$bac), ref = "none")
model.expanded.1 = plm(log(total_fatalities_rate) ~ year_of_observation + 
                                                    factor(bac) +
                                                    per_se_laws + 
                                                    primary_seatbelt_law + 
                                                    secondary_seatbelt_law + 
                                                    speed_limit_70plus + 
                                                    graduated_drivers_license_law + 
                                                    pct_population_14_to_24 +
                                                    unemployment_rate +
                                                    vehicle_miles_per_capita,
                                                    #log(pct_population_14_to_24) + 
                                                    #log(unemployment_rate) + 
                                                    #log(vehicle_miles_per_capita), 
                       data = data,
                       index = c("state", "year"),
                       effect = "individual", 
                       model = "pooling")
summary(model.expanded.1)
coeftest(model.expanded.1, vcov. = vcovHC, type = "HC1")
```

If it is appropriate, include transformations of these variables. Please carefully 
explain carefully your rationale, which should be based on your EDA, behind any 
transformation you made. If no transformation is made, explain why transformation 
is not needed. 

## Interpretation of Results 

###  How are the blood alcohol variables defined? Interpret the coefficients that you estimate for this concept. 

In the previous section,we have defined our treatment of the BAC variable as a 
factor with levels 'none', '10' and '8'. Here, in this model we consider "none" as 
the base level with no blood alcohol limit. The bac value of 0.08% is statistically 
significant and 0.1% is not statistically significant. The model suggests that 
setting a blood alcohol limit of 0.1% reduces the log fatality rate 
`r round(10^coef(model.expanded.1)['factor(bac)10'], 2)` times with respect to 
no BAC limit.  The model suggests that setting a blood alcohol limit of 0.08% 
reduces log fatality rate `r round(10^coef(model.expanded.1)['factor(bac)8'], 2)` 
times as compared to no BAC limit.
    
### Do *per se laws* have a negative effect on the fatality rate? 

We observe that per-se law is not a statistically significant parameter. The model 
suggests that having a per-se law reduces fatality rate `r round(10^coef(model.expanded.1)['per_se_laws'], 2)` 
times as compared to not having per-se law.

### Does having a primary seat belt law reduce fatality rates?

We notice that the seat belt law factors (primary_seatbelt_law andsecondary_seatbelt_law)
are not statistically significant. We also note that the implementation of the 
primary law leads to a `r round(10^coef(model.expanded.1)['primary_seatbelt_law'], 2)` 
times reduction in log fatality rate which is practically insignificant. 

# (15 points) State-Level Fixed Effects 

Re-estimate the **Expanded Model** using fixed effects at the state level. 

- What do you estimate for coefficients on the blood alcohol variables? How do 
the coefficients on the blood alcohol variables change, if at all? 
- What do you estimate for coefficients on per se laws? How do the coefficients 
on per se laws change, if at all? 
- What do you estimate for coefficients on primary seat-belt laws? How do the 
coefficients on primary seatbelt laws change, if at all? 

Model estimation for a fixed effect (within) model.
```{r, Fixed Effects models}
within.model <- plm(log(total_fatalities_rate) ~ year_of_observation + 
                                                 factor(bac) +
                                                 per_se_laws + 
                                                 primary_seatbelt_law + 
                                                 secondary_seatbelt_law + 
                                                 speed_limit_70plus + 
                                                 graduated_drivers_license_law + 
                                                 pct_population_14_to_24 +
                                                 unemployment_rate +
                                                 vehicle_miles_per_capita,
                                                 #log(pct_population_14_to_24) + 
                                                 #log(unemployment_rate) + 
                                                 #log(vehicle_miles_per_capita), 
                       data = data,
                       index = c("state", "year"),
                       effect = "individual", 
                       model = "within")
summary(within.model)
coeftest(within.model, vcov. = vcovHC, type = "HC1")
pFtest(within.model, model.expanded.1)
```

## Interpretation of the Model

```{r, echo=FALSE, warning=FALSE, results = 'asis'}
stargazer(model.expanded.1, 
          within.model,
          omit = c("year_of_observation"),
          column.labels = c("Pooled", "Within"),
          style = "qje", 
          type = "text",
          omit.stat = c("adj.rsq","f"), 
          header = TRUE,
          column.sep.width = "-10pt",
          single.row = TRUE, # to put coefficients and standard errors on same line
          no.space = TRUE # to remove the spaces after each line of coefficients
         )
```
Which set of estimates do you think is more reliable? Why do you think this? 

We find that BAC value of 0.1% is statistically significant parameters for both
pooled and within model, but BAC value of .08% is only statistically
significant for within model.

This within model suggests that setting a blood alcohol limit of 0.1% reduces
log fatality rate by `r round(10 ^ coef(within.model)['factor(bac)10'], 2)` times 
as compared to no BAC limit. Additionally, setting a blood alcohol limit of 0.08% 
reduces log fatality rate  by `r round(10 ^ coef(within.model)['factor(bac)8'], 2)` 
times as compared to no BAC limit. 

We note that per-se law is a statistically significant parameter for within model,
suggesting that having a per-se law reduces fatality rate `r round(10^coef(within.model)['per_se_laws'], 2)` 
as compared to not having per-se law which is less compared to the expanded OLS 
model which showed that having a per-se law reduces fatality rate 
`r round(10^coef(model.expanded.1)['per_se_laws'], 2)` times compared to not 
having per-se law.

The within model shows primary seatbelt law is a significant factorsuggesting 
`r round(10^coef(within.model)['primary_seatbelt_law'], 2)` unit decrease in 
fatality rate as compared to not having primary seat belt law, compared to the 
expanded OLS model for which the parameter was not statistically significant.

- What assumptions are needed in each of these models?  

### Fixed effect model assumptions

* For each 'i' the model is 
$y_{it} = \beta_1 x_{it1} + ... + \beta_k x_{itk} + a_i + u_{it}, t =1 ..T$
* We have a random sample from the cross section
* Each explanatory variable changes over time(for at least some time) and no 
perfect linear relationship exists between explanatory variables
* For each t, the expected value of the idiosyncratic error given the explanatory 
ariables in all time periods and the unobserved effect is zero: $E(u_{it}|X_i, a_i)=0$
* The variance of the difference errors, conditional on all explanatory variables, 
is constant $Var(\triangle u_{it} | X_i) = \sigma^2_u, t=2,....,T$. This is 
required for homoskedastic errors
* For all $t \neq s$, the differences in the idiosyncratic errors are 
uncorrelated(conditional on all explanatory variables). This is for serially 
uncorrelated residuals
 
- Are these assumptions reasonable in the current context?

> The assumption in an pooled OLS model is that the data is IID. Here in the 
data set, a sample of a large population is collected on different years. It is 
unlikely that a particular individual sample data point is measured twice. In 
such a circumstance a pooled OLS model would be applicable. 
> However in this data set, data granularity is at the state level and the same 
state is measured multiple times across years. This violates the assumption of 
IID in the pooled OLS. 
> A fixed effect model is then expected to be a better model in this scenario. 
We perform a F-test between the pooled and the fixed effect model to check for 
fixed effects. The null hypothesis is that there are no fixed effects and the 
alternate hypothesis is that there are fixed effects. We test against an alpha 
of 0.05.

```{r, model comparison between pooled and within model}
res <- pFtest(within.model, model.expanded.1)
```

With a p-value of `r as.numeric(res$p.value)` less than 0.05, we reject the null 
hypothesis of no fixed effects and this implies to include state and/or time 
fixed effects in our model. Therefore the fixed effect model is better for 
this scenario.

# (10 points) Consider a Random Effects Model 

Instead of estimating a fixed effects model, should you have estimated a random 
effects model?

```{r random-effect-model estimate, results='hide'}
random.effect.model <- plm(log(total_fatalities_rate) ~ year_of_observation + 
                                                        factor(bac) +
                                                        per_se_laws + 
                                                        primary_seatbelt_law + 
                                                        secondary_seatbelt_law + 
                                                        speed_limit_70plus + 
                                                        graduated_drivers_license_law + 
                                                        pct_population_14_to_24 +
                                                        unemployment_rate +
                                                        vehicle_miles_per_capita,
                                                        #log(pct_population_14_to_24) + 
                                                        #log(unemployment_rate) + 
                                                        #log(vehicle_miles_per_capita), 
                           data = data,
                           index = c("state", "year"),
                           model = "random")
summary(random.effect.model)
coeftest(random.effect.model, vcov. = vcovHC, type = "HC1")
```

- Please state the assumptions of a random effects model, and evaluate whether 
these assumptions are met in the data. 

> The first assumption of the random effect model is that there are no perfect 
linear relationships among the explanatory variables. 

```{r vif for RE model}
vif(random.effect.model)
```
**We see high values for percent_pop_aged_14_to_24, vehicle_miles_per_capita 
indicating the possible presence of multicollinearity in these variables.**

> The second assumption is that there is no correlation between the unobserved 
random and fixed effects and the explanatory variables. Using a random effects 
model imposes the error structure that the error term** $v_{it}$ **is 
equal to the sum of variation between groups and variation within groups onto the 
model residuals, allowing to properly specify the residuals and more efficiently 
estimate the coefficients of interest. This requires the assumption of independence 
between random effects and the other predictors in the model. The assumptions for 
the fixed effect model are discussed above, the additional assumption of independence 
of random effects and other predictors in the model is evaluated below. The test 
we run is the Hausman Test for fixed versus random effects. The null hypothesis 
is that the random effects model is acceptable while the alternative hypothesis 
is that there is correlation between residuals and predictors, meaning that we 
should use the FE model.**

We conduct a Hausman test for random vs. fixed effects using `phtest`. We perform 
this test with an $\alpha = 0.05$

```{r, model comparison between within and random efect moidels}
res <- phtest(within.model, random.effect.model)
```
With a p-value of `r res$p.value` less than $\alpha$, we reject the null 
hypothesis that random effects are appropriate, suggesting that we should use the 
fixed models. The random effects model is not likely to be consistent in this case.

>The third assumption is that of homoskedastic errors, which we can test below 
using the Breusch-Pagan Lagrange Multiplier for random effects. Null is no 
panel effect: 

```{r pcdtest for Random Effect Model}
plmtest(random.effect.model)
```

> Here we failed to reject the null and conclude that random effects is 
not appropriate.

- If the assumptions are, in fact, met in the data, then estimate a random effects 
model and interpret the coefficients of this model. Comment on how, if at all, 
the estimates from this model have changed compared to the fixed effects model. 

- If the assumptions are **not** met, then do not estimate the data. But, also 
comment on what the consequences would be if you were to *inappropriately* 
estimate a random effects model. Would your coefficient estimates be biased or not? 
Would your standard error estimates be biased or not? Or, would there be some other 
problem that might arise?

> As we have seen that the assumptions for random effect model are not met. If 
we were to inappropriately estimate a random effect model, we would be incorrectly 
assuming that the random effects and other predictors are independent of one another. 
This would lead to omitted variable bias as the correlation between the random 
effects and the explanatory variables of interest would not allow for accurate 
estimation of the coefficient. Standard errors will also be biased as we are 
assuming that the random effects, which are included in the error term, are 
incorrectly uncorrelated with the predictors - given that there is correlation, 
this will introduce bias into the standard errors.

# (10 points) Model Forecasts 

The COVID-19 pandemic dramatically changed patterns of driving. Find data 
(and include this data in your analysis, here) that includes some measure of 
vehicle miles driven in the US. Your data should at least cover the period from 
January 2018 to as current as possible. With this data, produce the following 
statements: 

- Comparing monthly miles driven in 2018 to the same months during the pandemic: 
  - What month demonstrated the largest decrease in driving? How much, in percentage 
  terms, lower was this driving? 
  - What month demonstrated the largest increase in driving? How much, in percentage 
  terms, higher was this driving? 

We have downloaded population data from https://fred.stlouisfed.org/series/POPTHM
and vehicle driven data from https://fred.stlouisfed.org/series/TRFVOLUSM227NFWA.
Population includes resident population plus armed forces overseas. The monthly 
estimate is the average of estimates for the first of the month and the first of 
the following month. Vehicle Miles Traveled and the 12-Month Moving Vehicle Miles 
Traveled series are created by appending the recent monthly figures from the 
FHWA’s Traffic Volume Trends to their Historic Monthly Vehicle Miles Traveled 
(VMT) data file.

We have defined pandemic period between March 2020 through March 2021 when the 
Covid vaccine came in the market .

```{r download external data}
fredr_set_key("cd565a10e83d56f9f1150d5a2c067e2a")
data.vhcl <- fredr(
                   series_id = "TRFVOLUSM227NFWA",
                   observation_start = as.Date("1990-01-01"),
                   observation_end = as.Date("2023-08-01")
                  ) %>% dplyr::select(date,value) %>% as_tsibble(index = date)
data.pop <- fredr(
                  series_id = "POPTHM",
                  observation_start = as.Date("1990-01-01"),
                  observation_end = as.Date("2023-08-01")
                 )%>% dplyr::select(date, value) %>% as_tsibble(index = date)

data.temp <- merge(x = data.vhcl, y = data.pop, by = "date")
data.temp$vehicle_miles_per_capita = data.temp$value.x / data.temp$value.y * 1000
colnames(data.temp) <- c('date','mileage','pop','vehicle_miles_per_capita')
data.vhcl.ml.per.capita <- data.temp[, c('date','mileage','pop','vehicle_miles_per_capita')]
data.vhcl.ml.per.capita$year = year(data.vhcl.ml.per.capita$date)
data.vhcl.ml.per.capita$month = month(data.vhcl.ml.per.capita$date)

data.pre.pandemic <- data.vhcl.ml.per.capita %>% filter(year == 2018)
data.pandemic <- data.vhcl.ml.per.capita %>% filter(year == 2020 | year == 2021)
data.pandemic.arranged <- data.pandemic %>% arrange(month)
mileage.diff = data.pre.pandemic$mileage - 
                                data.pandemic.arranged$mileage
vehicle_miles_per_capita.diff = data.pre.pandemic$vehicle_miles_per_capita - 
                                data.pandemic.arranged$vehicle_miles_per_capita

drive_pandemic <- data.pandemic %>% slice(3:15)
data.pandemic$group <- 'pandemic'
data.pre.pandemic$group <- 'pre_pandemic'
data.pandemic.pre.post.comparison <- rbind(data.pre.pandemic, data.pandemic)
pct.mileage.apr = scales::percent(data.pandemic.arranged$mileage[4] / data.pre.pandemic$mileage[4])
pct.mileage.may = scales::percent(data.pandemic.arranged$mileage[5] / data.pre.pandemic$mileage[5])

plot.orig = data.vhcl.ml.per.capita %>% ggplot(aes(x = date,
                                                   y = mileage)
                                                  ) + geom_line()

plot.comparison = ggplot(data.pandemic.pre.post.comparison, 
                         aes(x = month, 
                             y = mileage, 
                             group = group)) + 
                  geom_line(aes(color = group)) + 
                  scale_x_continuous(breaks = pretty_breaks()) +
                  ylab('Miles Driven') + 
                  ggtitle('Compariosn of Miles Driven in Pre and During Pandemic Months')

(plot.orig | plot.comparison)
```

From the plot above, we notice the sheer mileage drop in April during pandemic time.
We see that in April 2020, mileage was `r pct.mileage.apr` with respect to same time
in April 2018. In May, 2020, the mileage change was `r pct.mileage.may` with respect 
to same time in May 2018`

Now, use these changes in driving to make forecasts from your models. 

- Suppose that the number of miles driven per capita, increased by as much as the 
COVID boom. Using the FE estimates, what would the consequences be on the number 
of traffic fatalities? Please interpret the estimate.
- Suppose that the number of miles driven per capita, decreased by as much as the 
COVID bust. Using the FE estimates, what would the consequences be on the number 
of traffic fatalities? Please interpret the estimate.

>Now, use these changes in driving to make forecasts from the models.

```{r FE estimate on fatality with increase by COVID boom}
effect1 <- between.model$coefficients['log(miles_driven_per_capita)'] *
  (drive_pandemic$TRFVOLUSM227NFWA[4]/drive_pandemic$TRFVOLUSM227NFWA[3])
```

>If the number of miles driven per-capita increased by as much as the COVID boom, the consequences of traffic fatalities would be expected to be an increase of `r effect1` percent (in the log of the per-capita miles driven).

```{r FE estimate on fatality with decrese by COVID boom}
effect2 <- between.model$coefficients['log(miles_driven_per_capita)'] * 
  (-(1-drive_pandemic_month$TRFVOLUSM227NFWA[4]/drive_2018$TRFVOLUSM227NFWA[4]))
```
If the number of miles driven per capita increased by as much as the COVID boom, the consequences of traffic fatalities would be expected to be an increase of `r effect2` percent. 


# (5 points) Evaluate Error 

If there were serial correlation or heteroskedasticity in the idiosyncratic errors 
of the model, what would be the consequences on the estimators and their standard 
errors? Is there any serial correlation or heteroskedasticity? 

```{r}
pcdtest(model.expanded.1, test = "lm")
pcdtest(within.model, test = "lm")
pcdtest(random.effect.model, test = "lm")

pdwtest(model.expanded.1)
pdwtest(within.model)
pdwtest(random.effect.model)

pdwtest(model.expanded.1, order = 2)
pdwtest(within.model, order = 2)
pdwtest(random.effect.model, order = 2)
```