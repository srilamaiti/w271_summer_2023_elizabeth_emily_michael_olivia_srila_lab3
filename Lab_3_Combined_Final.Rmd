---
title: 'Lab 3: Panel Models'
subtitle: 'US Traffic Fatalities: 1980 - 2004'
output: 
  bookdown::pdf_document2: default
---

```{r load packages, echo=FALSE, message=FALSE, warning=FALSE}
library(GGally)
library(tibble)
library(lme4)
library(gridExtra)
library(reshape2)
library(mgcv)
library(plm)
library(lmtest)
library(aTSA)
library(urca)
library(tsibble)
library(dplyr)
library(fabletools)
library(tidyverse)
library(magrittr)
library(patchwork)
library(scales)
library(plyr)
library(tidyr)
library(ggplot2)
library(ggthemes)
library(lubridate)
library(forecast)
library(sandwich)
library(tseries)
library(vars)
library(jsonlite)
library(fable)
library(gtrendsR) 
library(zoo)
library(feasts)
library(thematic)
library(ggfortify)
library(fpp3)
require(knitr)
library(stargazer)
library(ggrepel)
knitr::opts_chunk$set(tidy = TRUE, tidy.opts = list(comment = FALSE))
knitr::opts_chunk$set(comment = " ")
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```


# U.S. traffic fatalities: 1980-2004

We are answering the following **causal** question: 

> **"Do changes in traffic laws affect traffic fatalities?"**  

```{r load data, echo = FALSE, include=FALSE}
library(data.table)

load(file="~/271/w271_summer_2023_elizabeth_emily_michael_olivia_srila_lab3/data/driving.RData", verbose = TRUE)

## please comment these calls in your work 
#head(data)
#desc
```

We have added state abbreviation in the data along with DC.
```{r, adding state abbriviation, warning=FALSE, message=FALSE}
# Adding state abbriviation

# sorting stat.abb from R isn't correct because it puts Alaska and Alabama 
# out of order

states.list <- c("AL","AK","AZ","AR","CA","CO","CT","DC","DE","FL","GA","HI",
                 "ID",
                 "IL","IN","IA","KS","KY","LA","ME","MD","MA","MI","MN","MS",
                 "MO","MT","NE","NV","NH","NJ","NM","NY","NC","ND","OH","OK",
                 "OR","PA","RI","SC","SD","TN","TX","UT","VT","VA","WA","WV",
                 "WI","WY")

states <- data.frame("index"=1:51,"abbr"=states.list)
data <- merge(data, states, by.x = "state", by.y = "index")
```

We then added year of observation in the data set.
```{r, adding year of observation, warning=FALSE, message=FALSE}
# year_of_observation
data <- data %>%
  mutate(year_of_observation = factor(year))
```


# (30 points, total) Build and Describe the Data 

### Load the data and produce useful features.

    - Produce a new variable, called `speed_limit` that re-encodes the data 
    that is in `sl55`, `sl65`, `sl70`, `sl75`, and `slnone`; 
    
```{r recode speed limit columns, warning=FALSE, message=FALSE}    
data <- data %>%
  mutate(speed_limit =
           case_when(
             data$sl55 > .5 ~ 55,
             data$sl65 > .5 ~ 65,
             data$sl70 > .5 ~ 70,
             data$sl75 > .5 ~ 75,
             data$slnone > .5 ~ 0,
             data$year == 1996 & data$state == 6 ~ 75,
             data$year == 1996 & data$state == 11 ~ 70,
             data$year == 1995 & data$state == 21 ~ 65,
             data$year == 1997 & data$state == 24 ~ 70,
             data$year == 1988 & data$state == 47 ~ 65
           ))

```

    - Produce a new variable, called `year_of_observation` that re-encodes the 
    data that is in `d80`, `d81`, ... , `d04`. 
    
```{r recode year, warning=FALSE, message=FALSE}    
data <- data %>% 
  mutate(year_of_observation =
           case_when(
             data$d80 == 1 ~ 1980,
             data$d81 == 1 ~ 1981,
             data$d82 == 1 ~ 1982,
             data$d83 == 1 ~ 1983,
             data$d84 == 1 ~ 1984,
             data$d85 == 1 ~ 1985,
             data$d86 == 1 ~ 1986,
             data$d87 == 1 ~ 1987,
             data$d88 == 1 ~ 1988,
             data$d89 == 1 ~ 1989,
             data$d90 == 1 ~ 1990,
             data$d91 == 1 ~ 1991,
             data$d92 == 1 ~ 1992,
             data$d93 == 1 ~ 1993,
             data$d94 == 1 ~ 1994,
             data$d95 == 1 ~ 1995,
             data$d96 == 1 ~ 1996,
             data$d97 == 1 ~ 1997,
             data$d98 == 1 ~ 1998,
             data$d99 == 1 ~ 1999,
             data$d00 == 1 ~ 2000,
             data$d01 == 1 ~ 2001,
             data$d02 == 1 ~ 2002,
             data$d03 == 1 ~ 2003,
             data$d04 == 1 ~ 2004,
             TRUE ~ 00000
           )
)
# data %>% filter(year_of_observation == 00000)   
```    
    
    - Produce a new variable for each of the other variables that are one-hot 
    encoded (i.e. `bac*` variable series). 

```{r recode other one-hot vars, warning=FALSE, message=FALSE}
data <- data %>%
  mutate(
    blood_alc =
      case_when(
        bac10 > .5 ~ .1,
        bac08 > .5 ~ .08,
        bac08 == 0 & bac10 == 0 ~ 0,
        bac10 > bac08 ~ .1,
        TRUE ~ .08
      )
  )

```

    - Rename these variables to sensible names that are legible to a reader of 
    your analysis. For example, the dependent variable as provided is called, 
    `totfatrte`. Pick something more sensible, like, `total_fatalities_rate`. 
    There are few enough of these variables to change, that you should change 
    them for all the variables in the data. (You will thank yourself later.)


```{r, renaming to meaningful columns, warning=FALSE, message=FALSE}
# rename the variables to sensible names
data <- data %>%
  dplyr::rename(
    "total_fatalities_rate"                      = "totfatrte",
    "minimum_drinking_age"                       = "minage",
    "zero_tolerance_law"                         = "zerotol",
    "state_population"                           = "statepop",
    "graduated_drivers_license_law"              = "gdl",
    "per_se_laws"                                = "perse",
    "total_traffic_fatalities"                   = "totfat",
    "total_nighttime_fatalities"                 = "nghtfat",
    "total_weekend_fatalities"                   = "wkndfat",
    "total_fatalities_per_100_million_miles"     = "totfatpvm",
    "nighttime_fatalities_per_100_million_miles" = "nghtfatpvm",
    "weekend_fatalities_per_100_million_miles"   = "wkndfatpvm",
    "nighttime_fatalities_rate"                  = "nghtfatrte",
    "weekend_fatalities_rate"                    = "wkndfatrte",
    "vehicle_miles"                              = "vehicmiles",
    "unemployment_rate"                          = "unem",
    "pct_population_14_to_24"                    = "perc14_24",
    "vehicle_miles_per_capita"                   = "vehicmilespc",
    "primary_seatbelt_law"                       = "sbprim",
    "secondary_seatbelt_law"                     = "sbsecon",
    "Abbreviation"                               = "abbr"
  ) 

# Check data
#data %>% glimpse()
```


### Description of the basic structure of the dataset. 
    
> The long-panel data being reviewed here has `r ncol(data)` columns and `r nrow(data)` 
row, where each row is returning metrics having to do with traffic incidents in 
each state (excluding Alaska and Hawaii) from the years 1980 through 2004, at an 
annual granularity. Specifically, we will be focusing on traffic fatalities in 
each state to see if regulations like blood alcohol limits, speed limits, and so 
on appear to impact the rate of fatalities per state. This dataset also includes 
columns that show how many of these fatalities happened while driving at night or 
on the weekend. The data was compiled by Donald G Freedman for the paper "Drunk 
living legislation and traffic fatalities:New evidence on BAC 08 laws" - Contemporary 
Economic Policy 2007. In the paper it is noted that "Fatality data are from the 
Fatality Analysis Reporting System (FARS) compiled by NHTSA. Data on traffic legislation 
for the years 1982—1999 were provided by Thomas Dee. Earlier data on legislation 
were taken from Zador et al. (1989) and later data on legislation from the National 
Center for Statistics and Analysis at the NHTSA Web site at http://www-nrd.nhtsa.dot.gov/departments/nrd-30/ncsa/. 
Data on graduated drivers’ licenses are taken from Dee, Grabowski, and Morrisey 
(2005). State unemployment rates are from Dee and the Bureau of Labor Statistics; 
age data are from the Bureau of the Census". **The outcome of interest, 
`total_fatalities_rate` is defined as the number of fatalities per 100,000 people.**
    
    
### EDA

> A thorough EDA is conducted on the dataset to explore the relationship of certain
variables at the state and aggregate level. First, data validity checks were conducted 
in order to determine that the observations across the states in the dataset were
consistent and repeated across all years without large gaps (code is commented out
to reduce output). This showed that the dataset did in fact have consistency and 
all observations were accounted for. We also verified that state numbers 2 and 13, 
which corresponded to Alaska and Hawaii, were indeed missing from the dataset.

```{r table of observations, warning=FALSE, message=FALSE}
#data %>% 
#  dplyr::select(year_of_observation, state) %>%
#  table()
```

```{r ensure consecutive data, warning=FALSE, message=FALSE}
#data %>%
#  is.pconsecutive()
#
#pdim(data)
```

> Next, we interrogated the total fatalities rate available in the data. A state
by state view of each is shown in **Figure 1**. An initial observation can be made
by studying this figure - states like Wyoming and New Jersey appear to stand out
from others as having high fatality rates. States like Connecticut and Rhode Island
appear to stand out as having low fatality rate. All states show a downward or 
flat trend in fatality over time.

```{r,warning=FALSE, message=FALSE, echo=FALSE, fig.height=9, fig.width=16, fig.cap="Fatality Rate by State (where data recorded) and Year Since 1980"}
data %>%
  ggplot(aes(x = year, y = total_fatalities_rate)) +
  geom_line() +
  geom_smooth() +
  facet_wrap(~ Abbreviation, nrow = 5, ncol=10) +
  labs(x = "Year",  y = "Fatality rate") +
  theme(legend.position = "none") +
  scale_x_continuous(breaks=c(1980, 1990, 2000))

```

> The mean fatality rate is reported in **Figure 2**. It shows that, over time and
across all the 48 contiguious states (plus DC), the fatality rate decreased between
1980 and 2004. Notably, there was a substantial decrease between 1980 and approximately
1993, with leveling off afterwards, from a high of >25 deaths per 100K to a low of 
~17 per 100K.

```{r, warning=FALSE, message=FALSE, echo=FALSE, fig.height=6, fig.width=12, fig.cap="Time Series of Average Fatality"}

mean_fat <- data %>%
  dplyr::group_by(year) %>%
  dplyr::summarise(mean = mean(total_fatalities_rate)) %>% mutate(year=lubridate::ymd(year, truncated=2L))

mean_fat <- mean_fat %>% as_tsibble(index=year)

mean_fat %>% autoplot(mean) + geom_smooth() +
  xlab("Year") +
  ylab("Mean Fatality Rate") +
  ggtitle("Mean Fatality Rate from 1980 - 2004")

```

> An additional study across states was done to see how states performed on an equal 
axis, which is shown in **Figure 3**. The major takeaway is a repeat of **Figure 1**, 
which is that Wyoming and New Jersey are states that appear to have higher rates of
fatality, while Connecticut, Rhode Island, New Mexico, and Idaho appear to be states
with lower fatality rates.


```{r,warning=FALSE,echo=FALSE, fig.height=6, fig.width=12, fig.cap="Fatality Rate by State on Common Axis"}


p2<- data %>%
  filter(as.integer(state) <= 12 ) %>%
  ggplot(aes(x = year, y = total_fatalities_rate)) +
  geom_line(aes(color = Abbreviation)) +
  labs(x = "Year",  y = "Fatality rate")+
  geom_label_repel(data = filter(data, as.integer(state) <= 12  & year == 1984),
                   aes(label = Abbreviation), nudge_x = .75,na.rm = TRUE) +
  ylim(0, 55) +
  theme(legend.position = "none")

p3<- data %>%
   filter(as.integer(state) > 12 & as.integer(state) <= 24 ) %>%
  ggplot(aes(x = year, y = total_fatalities_rate)) +
  geom_line(aes(color = Abbreviation)) +
  labs(x = "Year",  y = "Fatality rate")+
  geom_label_repel(data = filter(data, as.integer(state) > 12 & as.integer(state) <= 24  & year == 1984),
                   aes(label = Abbreviation), nudge_x = .75,na.rm = TRUE) +
  ylim(0, 55) +
  theme(legend.position = "none")

p4<- data %>%
  filter(as.integer(state) > 24 &as.integer(state) <= 36 ) %>%
  ggplot(aes(x = year, y = total_fatalities_rate)) +
  geom_line(aes(color = Abbreviation)) +
  labs(x = "Year",  y = "Fatality rate")+
  geom_label_repel(data = filter(data, as.integer(state) > 24 &as.integer(state) <= 36 & year == 1984),
                   aes(label = Abbreviation), nudge_x = .75,na.rm = TRUE) +
  ylim(0, 55) +
  theme(legend.position = "none")

p5<- data %>%
  filter(as.integer(state) > 36 ) %>%
  ggplot(aes(x = year, y = total_fatalities_rate)) +
  geom_line(aes(color = Abbreviation)) +
  labs(x = "Year",  y = "Fatality rate") +
  geom_label_repel(data = filter(data, as.integer(state) > 36 & year == 1984),aes(label = Abbreviation),
                   nudge_x = .75,na.rm = TRUE) +
  ylim(0, 55) +
  theme(legend.position = "none")

grid.arrange(p2,p3,p4, p5, nrow = 1, ncol = 4, top="Fatality Rate Across States")
```

> To explore the impact of other variables of interest at the aggregagate and state
level, we first used a scatterplot matrix to find baseline correlations between 
the variables when averaging across states. **Figure 4** shows the scatterplot matrix
of the variables, averaged across the states and DC. The variables with strong correlation
to the average fatality rate include vehicle miles driven per capita (-0.88), average
population (-0.857), and average percentage of the population consisting of individuals
aged 14-24 (0.909). Lessor variables included average unemployment and the average BAC.
The population 14-24 is interesting because it's known that young adults tend to
engage in riskier activties. 

```{r, warning=FALSE, message=FALSE, warning=FALSE, fig.width=12, fig.height=12, fig.cap="Scatterplot Matrix of Variables of Interest"}

all_means <- data %>%
  dplyr::group_by(year) %>%
  dplyr::summarise(
    avg_total_fatality_rate = mean(total_fatalities_rate), 
    avg_drinking_age = mean(minimum_drinking_age),
    avg_pop = mean(state_population), 
    avg_unemployment = mean(unemployment_rate), 
    avg_perc_pop = mean(`pct_population_14_to_24`),
    avg_speed_limit = mean(speed_limit), 
    avg_blood_alc_limit = mean(blood_alc),
    avg_vmd_percap = mean(vehicle_miles_per_capita)
  )

all_means %>% ggpairs() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```


```{r,warning=FALSE, message=FALSE,echo=FALSE, fig.height=6, fig.width=12, fig.cap="Vehicle Miles Driven Per Capita"}

p2<- data %>%
  filter(as.integer(state) <= 12 ) %>%
  ggplot(aes(x = year, y = vehicle_miles_per_capita)) +
  geom_line(aes(color = Abbreviation)) +
  labs(x = "Year",  y = "Vehicle Miles Per Capita")+
  geom_label_repel(data = filter(data, as.integer(state) <= 12  & year == 1984),
                   aes(label = Abbreviation), nudge_x = .75,na.rm = TRUE) +
  theme(legend.position = "none") + ylim(0,20000)

p3<- data %>%
   filter(as.integer(state) > 12 & as.integer(state) <= 24 ) %>%
  ggplot(aes(x = year, y = vehicle_miles_per_capita)) +
  geom_line(aes(color = Abbreviation)) +
  labs(x = "Year",  y = "Vehicle Miles Per Capita")+
  geom_label_repel(data = filter(data, as.integer(state) > 12 & as.integer(state) <= 24  & year == 1984),
                   aes(label = Abbreviation), nudge_x = .75,na.rm = TRUE) +
  theme(legend.position = "none") + ylim(0,20000)

p4<- data %>%
  filter(as.integer(state) > 24 &as.integer(state) <= 36 ) %>%
  ggplot(aes(x = year, y = vehicle_miles_per_capita)) +
  geom_line(aes(color = Abbreviation)) +
  labs(x = "Year",  y = "Vehicle Miles Per Capita")+
  geom_label_repel(data = filter(data, as.integer(state) > 24 &as.integer(state) <= 36 & year == 1984),
                   aes(label = Abbreviation), nudge_x = .75,na.rm = TRUE) +
  theme(legend.position = "none") + ylim(0,20000)

p5<- data %>%
  filter(as.integer(state) > 36 ) %>%
  ggplot(aes(x = year, y = vehicle_miles_per_capita)) +
  geom_line(aes(color = Abbreviation)) +
  labs(x = "Year",  y = "Vehicle Miles Per Capita") +
  geom_label_repel(data = filter(data, as.integer(state) > 36 & year == 1984),aes(label = Abbreviation),
                   nudge_x = .75,na.rm = TRUE) +
  theme(legend.position = "none") + ylim(0,20000)

grid.arrange(p2,p3,p4, p5, nrow = 1, ncol = 4)
```

```{r,warning=FALSE, message=FALSE,echo=FALSE, fig.height=6, fig.width=12, fig.cap="Percent of Population Aged 14-24"}

p2<- data %>%
  filter(as.integer(state) <= 12 ) %>%
  ggplot(aes(x = year, y = `pct_population_14_to_24`)) +
  geom_line(aes(color = Abbreviation)) +
  labs(x = "Year",  y = "Percent of Population 14-24")+
  geom_label_repel(data = filter(data, as.integer(state) <= 12  & year == 1984),
                   aes(label = Abbreviation), nudge_x = .75,na.rm = TRUE) +
  theme(legend.position = "none") + ylim(10,25)

p3<- data %>%
   filter(as.integer(state) > 12 & as.integer(state) <= 24 ) %>%
  ggplot(aes(x = year, y = `pct_population_14_to_24`)) +
  geom_line(aes(color = Abbreviation)) +
  labs(x = "Year",  y = "Percent of Population 14-24")+
  geom_label_repel(data = filter(data, as.integer(state) > 12 & as.integer(state) <= 24  & year == 1984),
                   aes(label = Abbreviation), nudge_x = .75,na.rm = TRUE) +
  theme(legend.position = "none") + ylim(10,25)

p4<- data %>%
  filter(as.integer(state) > 24 &as.integer(state) <= 36 ) %>%
  ggplot(aes(x = year, y = `pct_population_14_to_24`)) +
  geom_line(aes(color = Abbreviation)) +
  labs(x = "Year",  y = "Percent of Population 14-24")+
  geom_label_repel(data = filter(data, as.integer(state) > 24 &as.integer(state) <= 36 & year == 1984),
                   aes(label = Abbreviation), nudge_x = .75,na.rm = TRUE) +
  theme(legend.position = "none") + ylim(10,25)

p5<- data %>%
  filter(as.integer(state) > 36 ) %>%
  ggplot(aes(x = year, y = `pct_population_14_to_24`)) +
  geom_line(aes(color = Abbreviation)) +
  labs(x = "Year",  y = "Percent of Population 14-24") +
  geom_label_repel(data = filter(data, as.integer(state) > 36 & year == 1984),aes(label = Abbreviation),
                   nudge_x = .75,na.rm = TRUE) +
  theme(legend.position = "none") + ylim(10,25)

grid.arrange(p2,p3,p4, p5, nrow = 1, ncol = 4)
```

> To see how each state compares to the other, **Figure 5** and **Figure 6** are
displayed with shows the time series, for each state, of the vehicle miles driven
per capita and the percent of the population aged 14-24. Interesting, again Wyoming
stands out as a state where more vehicle miles are driven per capita than anywhere
else. New York is an outlier in the oppostie direction. Wyoming does not appear
to have a high number of adolescant adults, as a percentage of its population, shown
in **Figure 6**.


```{r,warning=FALSE, echo=FALSE, fig.height=6, fig.width=16, fig.cap="Box Plot of States - Sorted by Total Fatality Rate"}
b1 <- data %>%
  group_by(Abbreviation) %>%
  ggplot(aes(x = reorder(Abbreviation,total_fatalities_rate), y = total_fatalities_rate)) +
  geom_boxplot() +
  labs(x = "States",  y = "Fatality rate") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

b2 <- data %>%
  group_by(Abbreviation) %>%
  ggplot(aes(x = reorder(Abbreviation,total_fatalities_rate), y = `pct_population_14_to_24`)) +
  geom_boxplot() +
  labs(x = "States",  y = "Percent pop 14-24") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

b3 <- data %>%
  group_by(Abbreviation) %>%
  ggplot(aes(x = reorder(Abbreviation,total_fatalities_rate), y = minimum_drinking_age)) +
  geom_boxplot() +
  labs(x = "States",  y = "Min Drinking Age") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

b4 <- data %>%
  group_by(Abbreviation) %>%
  ggplot(aes(x = reorder(Abbreviation,total_fatalities_rate), y = unemployment_rate)) +
  geom_boxplot() +
  labs(x = "States",  y = "Unemployment Rate") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

b5 <- data %>%
  group_by(Abbreviation) %>%
  ggplot(aes(x = reorder(Abbreviation,total_fatalities_rate), y = blood_alc)) +
  geom_boxplot() +
  labs(x = "States",  y = "Avg. Blood Alcohol Content") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

b6 <- data %>%
  group_by(Abbreviation) %>%
  ggplot(aes(x = reorder(Abbreviation,total_fatalities_rate), y = vehicle_miles_per_capita)) +
  geom_boxplot() +
  labs(x = "States",  y = "Vehicle Miles per Capita") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
        
grid.arrange(b1, b6, b3, b4, b2, b5, ncol=3, nrow=2)
```

> Finally, a boxplot of the distribution across all years is built for a subset
of the variables. This is presented in **Figure 7**. As expected, the state with
the highest average fatality rate is Wyoming, and the state with the smallest is 
Rhode Island. When viewing the vehicle miles per capita, the relationship is striking -
Wyoming is also the state with the highest average vehicle miles driven per capita
across the years surveyed. The population aged 14-24 is also presented in this 
fashion but no major observations can be determined. We will the variables that
we've identified in the EDA as important to build the causal models.

# Preliminary Model


### Linear Model 

```{r preliminary model, message=FALSE, warning=FALSE}

```

### Discussion on Linear Model


# Expanded Model 


### Modeling

```{r expanded model, message=FALSE, warning=FALSE}

```


# State-Level Fixed Effects 

### Modeling

```{r state level fixed effects, message=FALSE, warning=FALSE, results='asis'}

within.model <- plm(
  total_fatalities_rate ~ as.numeric(year) + 
    blood_alc +
    per_se_laws + 
    primary_seatbelt_law + 
    speed_limit + 
    graduated_drivers_license_law +               
    log(`pct_population_14_to_24`) + 
    log(unemployment_rate) + 
    log(vehicle_miles_per_capita), 
  data = data, 
  index=c("state", "year"), 
  model="within"
)


pool.model <- plm(
  total_fatalities_rate ~ as.numeric(year) + 
    blood_alc +
    per_se_laws + 
    primary_seatbelt_law + 
    speed_limit + 
    graduated_drivers_license_law +               
    log(`pct_population_14_to_24`) + 
    log(unemployment_rate) + 
    log(vehicle_miles_per_capita), 
  data = data, 
  index=c("state", "year"),
  model="pooling"
)

fd.model <- plm(total_fatalities_rate ~ as.numeric(year) + 
    blood_alc +
    per_se_laws + 
    primary_seatbelt_law + 
    speed_limit + 
    graduated_drivers_license_law +               
    log(`pct_population_14_to_24`) + 
    log(unemployment_rate) + 
    log(vehicle_miles_per_capita), 
  data = data, 
  index=c("state", "year"), 
  model="fd"
)

between.model <- plm(
  total_fatalities_rate ~ as.numeric(year) +  
    blood_alc +
    per_se_laws + 
    primary_seatbelt_law + 
    speed_limit + 
    graduated_drivers_license_law +               
    log(`pct_population_14_to_24`) + 
    log(unemployment_rate) + 
    log(vehicle_miles_per_capita), 
  data = data, 
  index=c("state", "year"),
  model="between"
)

random.model <- plm(
  total_fatalities_rate ~ as.numeric(year) +  
    blood_alc +
    per_se_laws + 
    primary_seatbelt_law + 
    speed_limit + 
    graduated_drivers_license_law +               
    log(`pct_population_14_to_24`) + 
    log(unemployment_rate) + 
    log(vehicle_miles_per_capita), 
  data = data, 
  index=c("state", "year"),
  model="random"
)


stargazer(pool.model, fd.model, between.model, within.model, random.model, 
          type = "latex", header=FALSE,
          omit.stat = c("ser","f","adj.rsq"), dep.var.labels = "",
          column.labels = c("Pooled", "FD", "Between", "Within", "Random"),
          title="Comparison of Models")
```

> State level fixed effect models are estimated using the `plm` function in R, which
takes advantage of the panel data structure for model estimations. A *within*, *pooling*,
*first difference*, *between*, and *random effects* model are estimated and compared
in **Table 1**.

### Model analysis

```{r message=FALSE, warning=FALSE}
pFtest(within.model, pool.model)
```

> A pFtest is conducted to determine where state and time fixed effects should be included. 
Our pFtest returns a significant p value, meaning that we reject the null hypothesis. 
This means we should include the state and time fixed effects in our model.

```{r message=FALSE, warning=FALSE}
pwfdtest(fd.model, data=data, index=c("state", "year"), h0="fe")
```

```{r message=FALSE, warning=FALSE}
pwfdtest(fd.model, data=data, index=c("state", "year"), h0="fd")
```

```{r message=FALSE, warning=FALSE}
phtest(within.model, random.model)
```

### Blood Alcohol Affects

> The fixed effect model estimates that blood alcohol has a coefficient of -6.163. In action, this means that for a unit increase in blood alcohol limit, the fatality rate will decrease by approximately 6 fatalities per 100,000 people. However in our case, blood alcohol is measured in hundreths and tenths rather than in unitary increments, so a full unit increase isn't likely to take place. In the Pooled model, the coefficient for blood alcohol is -8.736, which is the largest impact out of the five models. The coefficient for the First Difference model is -3.092, the smallest impact of the models. The between model has a similar coefficient of -3.313, and lastly the Random Effects model has a coefficient of -6.610, very simlilar to the Fixed Effect Model. All of these coefficients are significant.

### Per se Laws

> The Fixed Effect model estimates that the coefficient for the per-se parameter is -1.219. In our data, the per se parameter is a one-hot-encoded variable with some values being between zero and one, in cases where states took on the law mid-year. In the case where a state does have per se laws in a given year, our model estimates that the rate of fatalities will decrease by 1.2 per 100,000 people. The is the largest impact of the five models, with the coefficients of the Pooled Effect, First Difference, Between, and Random Effect being -0.881, -0.669, -0.467, and -1.178 respectively. All of these coefficients are significant.

### Primary Seat Belt Laws

> The primary seat belt law variable is also a Boolean, indicating whether a state uses primary seat belt laws or secondary. For the Pooled model, the coefficient is not significant and is -0.322, a fairly small impact. Similarly, the coefficient for the First Difference model is -0.201, and the coefficient for the Between model is -0.491. This variable is not significant in either model. However, the coefficients for both the Fixed Effect and Random Effect models are both significant, being -0.867 and -0.833 respectively. 

### Reliability of Estimates

> The Wooldridge first-difference test for serial correlation in panels returns significant p values for both the Fixed Effect (within) and First Difference models, however the p value for the Fixed Effect model is far more significant. This leads us to believe that the Fixed Effect model is more reliable than the First Difference and Pooled models. Moving on, the outcome of the Hausmen test is not significant, meaning we do not reject the null hypothesis that random effects are appropriate, suggesting that we should not use the Fixed Effect model. Because of these outcomes, the estimates created by the Random Effects model will be the most reliable.

### Model Assumptions 

### Are the assumptions reasonable?


# (10 points) Consider a Random Effects Model 

Instead of estimating a fixed effects model, should you have estimated a random effects model?

- Please state the assumptions of a random effects model, and evaluate whether these assumptions are met in the data. 
- If the assumptions are, in fact, met in the data, then estimate a random effects model and interpret the coefficients of this model. Comment on how, if at all, the estimates from this model have changed compared to the fixed effects model. 
- If the assumptions are **not** met, then do not estimate the data. But, also comment on what the consequences would be if you were to *inappropriately* estimate a random effects model. Would your coefficient estimates be biased or not? Would your standard error estimates be biased or not? Or, would there be some other problem that might arise?

# (10 points) Model Forecasts 

The COVID-19 pandemic dramatically changed patterns of driving. Find data (and include this data in your analysis, here) that includes some measure of vehicle miles driven in the US. Your data should at least cover the period from January 2018 to as current as possible. With this data, produce the following statements: 

- Comparing monthly miles driven in 2018 to the same months during the pandemic: 
  - What month demonstrated the largest decrease in driving? How much, in percentage terms, lower was this driving? 
  - What month demonstrated the largest increase in driving? How much, in percentage terms, higher was this driving? 
  
Now, use these changes in driving to make forecasts from your models. 

- Suppose that the number of miles driven per capita, increased by as much as the COVID boom. Using the FE estimates, what would the consequences be on the number of traffic fatalities? Please interpret the estimate.
- Suppose that the number of miles driven per capita, decreased by as much as the COVID bust. Using the FE estimates, what would the consequences be on the number of traffic fatalities? Please interpret the estimate.

# (5 points) Evaluate Error 

If there were serial correlation or heteroskedasticity in the idiosyncratic errors of the model, what would be the consequences on the estimators and their standard errors? Is there any serial correlation or heteroskedasticity? 