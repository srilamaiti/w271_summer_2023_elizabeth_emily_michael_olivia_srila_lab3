---
title: 'Lab 3: Panel Models'
subtitle: 'US Traffic Fatalities: 1980 - 2004'
output: 
  bookdown::pdf_document2: default
---

```{r load packages, echo=FALSE, message=FALSE, warning=FALSE}
library(GGally)
library(tibble)
library(lme4)
library(gridExtra)
library(reshape2)
library(mgcv)
library(plm)
library(lmtest)
library(aTSA)
library(urca)
library(tsibble)
library(dplyr)
library(fabletools)
library(tidyverse)
library(magrittr)
library(patchwork)
library(scales)
library(plyr)
library(tidyr)
library(ggplot2)
library(ggthemes)
library(lubridate)
library(forecast)
library(sandwich)
library(tseries)
library(vars)
library(jsonlite)
library(fable)
library(gtrendsR) 
library(zoo)
library(feasts)
library(thematic)
library(ggfortify)
library(fpp3)
require(knitr)
library(stargazer)
library(ggrepel)
knitr::opts_chunk$set(tidy = TRUE, tidy.opts = list(comment = FALSE))
knitr::opts_chunk$set(comment = " ")
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```


# U.S. traffic fatalities: 1980-2004

We are answering the following **causal** question: 

> **"Do changes in traffic laws affect traffic fatalities?"**  

```{r load data, echo = FALSE, include=FALSE}
library(data.table)

load(file="~/271/w271_summer_2023_elizabeth_emily_michael_olivia_srila_lab3/data/driving.RData", verbose = TRUE)

## please comment these calls in your work 
#head(data)
#desc
```

### Preliminary Data Look

We have added state abbreviation in the data along with DC.
```{r, adding state abbriviation, warning=FALSE, message=FALSE}
# Adding state abbriviation

# sorting stat.abb from R isn't correct because it puts Alaska and Alabama 
# out of order

states.list <- c("AL","AK","AZ","AR","CA","CO","CT","DC","DE","FL","GA","HI",
                 "ID",
                 "IL","IN","IA","KS","KY","LA","ME","MD","MA","MI","MN","MS",
                 "MO","MT","NE","NV","NH","NJ","NM","NY","NC","ND","OH","OK",
                 "OR","PA","RI","SC","SD","TN","TX","UT","VT","VA","WA","WV",
                 "WI","WY")

states <- data.frame("index"=1:51,"abbr"=states.list)
data <- merge(data, states, by.x = "state", by.y = "index")
```

We then added year of observation in the data set.
```{r, adding year of observation, warning=FALSE, message=FALSE}
# year_of_observation
data <- data %>%
  mutate(year_of_observation = factor(year))
```


# Build and Describe the Data 

### Load the data and produce useful features.

Producing a new variable, called `speed_limit` that re-encodes the data 
that is in `sl55`, `sl65`, `sl70`, `sl75`, and `slnone`;.
    
```{r recode speed limit columns, warning=FALSE, message=FALSE}    
data <- data %>%
  mutate(speed_limit =
           case_when(
             data$sl55 > .5 ~ 55,
             data$sl65 > .5 ~ 65,
             data$sl70 > .5 ~ 70,
             data$sl75 > .5 ~ 75,
             data$slnone > .5 ~ 0,
             data$year == 1996 & data$state == 6 ~ 75,
             data$year == 1996 & data$state == 11 ~ 70,
             data$year == 1995 & data$state == 21 ~ 65,
             data$year == 1997 & data$state == 24 ~ 70,
             data$year == 1988 & data$state == 47 ~ 65
           ))

```

Producing a new variable, called `year_of_observation` that re-encodes the 
data that is in `d80`, `d81`, ... , `d04`. 
    
```{r recode year, warning=FALSE, message=FALSE}    
data <- data %>% 
  mutate(year_of_observation =
           case_when(
             data$d80 == 1 ~ 1980,
             data$d81 == 1 ~ 1981,
             data$d82 == 1 ~ 1982,
             data$d83 == 1 ~ 1983,
             data$d84 == 1 ~ 1984,
             data$d85 == 1 ~ 1985,
             data$d86 == 1 ~ 1986,
             data$d87 == 1 ~ 1987,
             data$d88 == 1 ~ 1988,
             data$d89 == 1 ~ 1989,
             data$d90 == 1 ~ 1990,
             data$d91 == 1 ~ 1991,
             data$d92 == 1 ~ 1992,
             data$d93 == 1 ~ 1993,
             data$d94 == 1 ~ 1994,
             data$d95 == 1 ~ 1995,
             data$d96 == 1 ~ 1996,
             data$d97 == 1 ~ 1997,
             data$d98 == 1 ~ 1998,
             data$d99 == 1 ~ 1999,
             data$d00 == 1 ~ 2000,
             data$d01 == 1 ~ 2001,
             data$d02 == 1 ~ 2002,
             data$d03 == 1 ~ 2003,
             data$d04 == 1 ~ 2004,
             TRUE ~ 00000
           )
)
# data %>% filter(year_of_observation == 00000)   
```    
    
Producing a new variable for each of the other variables that are one-hot 
encoded (i.e. `bac*` variable series). 

```{r recode other one-hot vars, warning=FALSE, message=FALSE}
data <- data %>%
  mutate(
    blood_alc =
      case_when(
        bac10 > .5 ~ .1,
        bac08 > .5 ~ .08,
        bac08 == 0 & bac10 == 0 ~ 0,
        bac10 > bac08 ~ .1,
        TRUE ~ .08
      )
  )

```

Renaming the variables to more legible names.

```{r, renaming to meaningful columns, warning=FALSE, message=FALSE}
# rename the variables to sensible names
data <- data %>%
  dplyr::rename(
    "total_fatalities_rate"                      = "totfatrte",
    "minimum_drinking_age"                       = "minage",
    "zero_tolerance_law"                         = "zerotol",
    "state_population"                           = "statepop",
    "graduated_drivers_license_law"              = "gdl",
    "per_se_laws"                                = "perse",
    "total_traffic_fatalities"                   = "totfat",
    "total_nighttime_fatalities"                 = "nghtfat",
    "total_weekend_fatalities"                   = "wkndfat",
    "total_fatalities_per_100_million_miles"     = "totfatpvm",
    "nighttime_fatalities_per_100_million_miles" = "nghtfatpvm",
    "weekend_fatalities_per_100_million_miles"   = "wkndfatpvm",
    "nighttime_fatalities_rate"                  = "nghtfatrte",
    "weekend_fatalities_rate"                    = "wkndfatrte",
    "vehicle_miles"                              = "vehicmiles",
    "unemployment_rate"                          = "unem",
    "pct_population_14_to_24"                    = "perc14_24",
    "vehicle_miles_per_capita"                   = "vehicmilespc",
    "primary_seatbelt_law"                       = "sbprim",
    "secondary_seatbelt_law"                     = "sbsecon",
    "Abbreviation"                               = "abbr"
  ) 

# Check data
#data %>% glimpse()
```


### Description of the basic structure of the dataset. 
    
> The long-panel data being reviewed here has `r ncol(data)` columns and `r nrow(data)` 
row, where each row is returning metrics having to do with traffic incidents in 
each state (excluding Alaska and Hawaii) from the years 1980 through 2004, at an 
annual granularity. Specifically, we will be focusing on traffic fatalities in 
each state to see if regulations like blood alcohol limits, speed limits, and so 
on appear to impact the rate of fatalities per state. This dataset also includes 
columns that show how many of these fatalities happened while driving at night or 
on the weekend. The data was compiled by Donald G Freedman for the paper "Drunk 
living legislation and traffic fatalities:New evidence on BAC 08 laws" - Contemporary 
Economic Policy 2007. In the paper it is noted that "Fatality data are from the 
Fatality Analysis Reporting System (FARS) compiled by NHTSA. Data on traffic legislation 
for the years 1982—1999 were provided by Thomas Dee. Earlier data on legislation 
were taken from Zador et al. (1989) and later data on legislation from the National 
Center for Statistics and Analysis at the NHTSA Web site at http://www-nrd.nhtsa.dot.gov/departments/nrd-30/ncsa/. 
Data on graduated drivers’ licenses are taken from Dee, Grabowski, and Morrisey 
(2005). State unemployment rates are from Dee and the Bureau of Labor Statistics; 
age data are from the Bureau of the Census". **The outcome of interest, 
`total_fatalities_rate` is defined as the number of fatalities per 100,000 people.**
    
    
### EDA

> A thorough EDA is conducted on the dataset to explore the relationship of certain
variables at the state and aggregate level. First, data validity checks were conducted 
in order to determine that the observations across the states in the dataset were
consistent and repeated across all years without large gaps (code is commented out
to reduce output). This showed that the dataset did in fact have consistency and 
all observations were accounted for. We also verified that state numbers 2 and 13, 
which corresponded to Alaska and Hawaii, were indeed missing from the dataset.

```{r table of observations, warning=FALSE, message=FALSE}
#data %>% 
#  dplyr::select(year_of_observation, state) %>%
#  table()
```

```{r ensure consecutive data, warning=FALSE, message=FALSE}
#data %>%
#  is.pconsecutive()
#
#pdim(data)
```

> Next, we interrogated the total fatalities rate available in the data. A state
by state view of each is shown in **Figure 1**. An initial observation can be made
by studying this figure - states like Wyoming and New Jersey appear to stand out
from others as having high fatality rates. States like Connecticut and Rhode Island
appear to stand out as having low fatality rate. All states show a downward or 
flat trend in fatality over time.

```{r,warning=FALSE, message=FALSE, echo=FALSE, fig.height=9, fig.width=16, fig.cap="Fatality Rate by State (where data recorded) and Year Since 1980"}
data %>%
  ggplot(aes(x = year, y = total_fatalities_rate)) +
  geom_line() +
  geom_smooth() +
  facet_wrap(~ Abbreviation, nrow = 5, ncol=10) +
  labs(x = "Year",  y = "Fatality rate") +
  theme(legend.position = "none") +
  scale_x_continuous(breaks=c(1980, 1990, 2000))

```

> The mean fatality rate is reported in **Figure 2**. It shows that, over time and
across all the 48 contiguious states (plus DC), the fatality rate decreased between
1980 and 2004. Notably, there was a substantial decrease between 1980 and approximately
1993, with leveling off afterwards, from a high of >25 deaths per 100K to a low of 
~17 per 100K.

```{r, warning=FALSE, message=FALSE, echo=FALSE, fig.height=6, fig.width=12, fig.cap="Time Series of Average Fatality"}

mean_fat <- data %>%
  dplyr::group_by(year) %>%
  dplyr::summarise(mean = mean(total_fatalities_rate)) %>% mutate(year=lubridate::ymd(year, truncated=2L))

mean_fat <- mean_fat %>% as_tsibble(index=year)

mean_fat %>% autoplot(mean) + geom_smooth() +
  xlab("Year") +
  ylab("Mean Fatality Rate") +
  ggtitle("Mean Fatality Rate from 1980 - 2004")

```

> An additional study across states was done to see how states performed on an equal 
axis, which is shown in **Figure 3**. The major takeaway is a repeat of **Figure 1**, 
which is that Wyoming and New Jersey are states that appear to have higher rates of
fatality, while Connecticut, Rhode Island, New Mexico, and Idaho appear to be states
with lower fatality rates.


```{r,warning=FALSE,echo=FALSE, fig.height=6, fig.width=12, fig.cap="Fatality Rate by State on Common Axis"}


p2<- data %>%
  filter(as.integer(state) <= 12 ) %>%
  ggplot(aes(x = year, y = total_fatalities_rate)) +
  geom_line(aes(color = Abbreviation)) +
  labs(x = "Year",  y = "Fatality rate")+
  geom_label_repel(data = filter(data, as.integer(state) <= 12  & year == 1984),
                   aes(label = Abbreviation), nudge_x = .75,na.rm = TRUE) +
  ylim(0, 55) +
  theme(legend.position = "none")

p3<- data %>%
   filter(as.integer(state) > 12 & as.integer(state) <= 24 ) %>%
  ggplot(aes(x = year, y = total_fatalities_rate)) +
  geom_line(aes(color = Abbreviation)) +
  labs(x = "Year",  y = "Fatality rate")+
  geom_label_repel(data = filter(data, as.integer(state) > 12 & as.integer(state) <= 24  & year == 1984),
                   aes(label = Abbreviation), nudge_x = .75,na.rm = TRUE) +
  ylim(0, 55) +
  theme(legend.position = "none")

p4<- data %>%
  filter(as.integer(state) > 24 &as.integer(state) <= 36 ) %>%
  ggplot(aes(x = year, y = total_fatalities_rate)) +
  geom_line(aes(color = Abbreviation)) +
  labs(x = "Year",  y = "Fatality rate")+
  geom_label_repel(data = filter(data, as.integer(state) > 24 &as.integer(state) <= 36 & year == 1984),
                   aes(label = Abbreviation), nudge_x = .75,na.rm = TRUE) +
  ylim(0, 55) +
  theme(legend.position = "none")

p5<- data %>%
  filter(as.integer(state) > 36 ) %>%
  ggplot(aes(x = year, y = total_fatalities_rate)) +
  geom_line(aes(color = Abbreviation)) +
  labs(x = "Year",  y = "Fatality rate") +
  geom_label_repel(data = filter(data, as.integer(state) > 36 & year == 1984),aes(label = Abbreviation),
                   nudge_x = .75,na.rm = TRUE) +
  ylim(0, 55) +
  theme(legend.position = "none")

grid.arrange(p2,p3,p4, p5, nrow = 1, ncol = 4, top="Fatality Rate Across States")
```

> To explore the impact of other variables of interest at the aggregagate and state
level, we first used a scatterplot matrix to find baseline correlations between 
the variables when averaging across states. **Figure 4** shows the scatterplot matrix
of the variables, averaged across the states and DC. The variables with strong correlation
to the average fatality rate include vehicle miles driven per capita (-0.88), average
population (-0.857), and average percentage of the population consisting of individuals
aged 14-24 (0.909). Lessor variables included average unemployment and the average BAC.
The population 14-24 is interesting because it's known that young adults tend to
engage in riskier activties. 

```{r, warning=FALSE, message=FALSE, warning=FALSE, fig.width=12, fig.height=12, fig.cap="Scatterplot Matrix of Variables of Interest"}

all_means <- data %>%
  dplyr::group_by(year) %>%
  dplyr::summarise(
    avg_total_fatality_rate = mean(total_fatalities_rate), 
    avg_drinking_age = mean(minimum_drinking_age),
    avg_pop = mean(state_population), 
    avg_unemployment = mean(unemployment_rate), 
    avg_perc_pop = mean(`pct_population_14_to_24`),
    avg_speed_limit = mean(speed_limit), 
    avg_blood_alc_limit = mean(blood_alc),
    avg_vmd_percap = mean(vehicle_miles_per_capita)
  )

all_means %>% ggpairs() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```


```{r,warning=FALSE, message=FALSE,echo=FALSE, fig.height=6, fig.width=12, fig.cap="Vehicle Miles Driven Per Capita"}

p2<- data %>%
  filter(as.integer(state) <= 12 ) %>%
  ggplot(aes(x = year, y = vehicle_miles_per_capita)) +
  geom_line(aes(color = Abbreviation)) +
  labs(x = "Year",  y = "Vehicle Miles Per Capita")+
  geom_label_repel(data = filter(data, as.integer(state) <= 12  & year == 1984),
                   aes(label = Abbreviation), nudge_x = .75,na.rm = TRUE) +
  theme(legend.position = "none") + ylim(0,20000)

p3<- data %>%
   filter(as.integer(state) > 12 & as.integer(state) <= 24 ) %>%
  ggplot(aes(x = year, y = vehicle_miles_per_capita)) +
  geom_line(aes(color = Abbreviation)) +
  labs(x = "Year",  y = "Vehicle Miles Per Capita")+
  geom_label_repel(data = filter(data, as.integer(state) > 12 &
                                   as.integer(state) <= 24  & year == 1984),
                   aes(label = Abbreviation), nudge_x = .75,na.rm = TRUE) +
  theme(legend.position = "none") + ylim(0,20000)

p4<- data %>%
  filter(as.integer(state) > 24 &as.integer(state) <= 36 ) %>%
  ggplot(aes(x = year, y = vehicle_miles_per_capita)) +
  geom_line(aes(color = Abbreviation)) +
  labs(x = "Year",  y = "Vehicle Miles Per Capita")+
  geom_label_repel(data = filter(data, as.integer(state) > 24 & 
                                   as.integer(state) <= 36 & year == 1984),
                   aes(label = Abbreviation), nudge_x = .75,na.rm = TRUE) +
  theme(legend.position = "none") + ylim(0,20000)

p5<- data %>%
  filter(as.integer(state) > 36 ) %>%
  ggplot(aes(x = year, y = vehicle_miles_per_capita)) +
  geom_line(aes(color = Abbreviation)) +
  labs(x = "Year",  y = "Vehicle Miles Per Capita") +
  geom_label_repel(data = filter(data, as.integer(state) > 36 &
                                   year == 1984),aes(label = Abbreviation),
                   nudge_x = .75,na.rm = TRUE) +
  theme(legend.position = "none") + ylim(0,20000)

grid.arrange(p2,p3,p4, p5, nrow = 1, ncol = 4)
```

```{r,warning=FALSE, message=FALSE,echo=FALSE, fig.height=6, fig.width=12, fig.cap="Percent of Population Aged 14-24"}

p2<- data %>%
  filter(as.integer(state) <= 12 ) %>%
  ggplot(aes(x = year, y = `pct_population_14_to_24`)) +
  geom_line(aes(color = Abbreviation)) +
  labs(x = "Year",  y = "Percent of Population 14-24")+
  geom_label_repel(data = filter(data, as.integer(state) <= 12  & year == 1984),
                   aes(label = Abbreviation), nudge_x = .75,na.rm = TRUE) +
  theme(legend.position = "none") + ylim(10,25)

p3<- data %>%
   filter(as.integer(state) > 12 & as.integer(state) <= 24 ) %>%
  ggplot(aes(x = year, y = `pct_population_14_to_24`)) +
  geom_line(aes(color = Abbreviation)) +
  labs(x = "Year",  y = "Percent of Population 14-24")+
  geom_label_repel(data = filter(data, as.integer(state) > 12 &
                                   as.integer(state) <= 24  & year == 1984),
                   aes(label = Abbreviation), nudge_x = .75,na.rm = TRUE) +
  theme(legend.position = "none") + ylim(10,25)

p4<- data %>%
  filter(as.integer(state) > 24 &as.integer(state) <= 36 ) %>%
  ggplot(aes(x = year, y = `pct_population_14_to_24`)) +
  geom_line(aes(color = Abbreviation)) +
  labs(x = "Year",  y = "Percent of Population 14-24")+
  geom_label_repel(data = filter(data, as.integer(state) > 24 &
                                   as.integer(state) <= 36 & year == 1984),
                   aes(label = Abbreviation), nudge_x = .75,na.rm = TRUE) +
  theme(legend.position = "none") + ylim(10,25)

p5<- data %>%
  filter(as.integer(state) > 36 ) %>%
  ggplot(aes(x = year, y = `pct_population_14_to_24`)) +
  geom_line(aes(color = Abbreviation)) +
  labs(x = "Year",  y = "Percent of Population 14-24") +
  geom_label_repel(data = filter(data, as.integer(state) > 36 &
                                   year == 1984),aes(label = Abbreviation),
                   nudge_x = .75,na.rm = TRUE) +
  theme(legend.position = "none") + ylim(10,25)

grid.arrange(p2,p3,p4, p5, nrow = 1, ncol = 4)
```

> To see how each state compares to the other, **Figure 5** and **Figure 6** are
displayed with shows the time series, for each state, of the vehicle miles driven
per capita and the percent of the population aged 14-24. Interesting, again Wyoming
stands out as a state where more vehicle miles are driven per capita than anywhere
else. New York is an outlier in the oppostie direction. Wyoming does not appear
to have a high number of adolescant adults, as a percentage of its population, shown
in **Figure 6**.


```{r,warning=FALSE, echo=FALSE, fig.height=6, fig.width=16, fig.cap="Box Plot of States - Sorted by Total Fatality Rate"}
b1 <- data %>%
  group_by(Abbreviation) %>%
  ggplot(aes(x = reorder(Abbreviation,total_fatalities_rate), 
             y = total_fatalities_rate)) +
  geom_boxplot() +
  labs(x = "States",  y = "Fatality rate") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

b2 <- data %>%
  group_by(Abbreviation) %>%
  ggplot(aes(x = reorder(Abbreviation,total_fatalities_rate), 
             y = `pct_population_14_to_24`)) +
  geom_boxplot() +
  labs(x = "States",  y = "Percent pop 14-24") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

b3 <- data %>%
  group_by(Abbreviation) %>%
  ggplot(aes(x = reorder(Abbreviation,total_fatalities_rate), 
             y = minimum_drinking_age)) +
  geom_boxplot() +
  labs(x = "States",  y = "Min Drinking Age") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

b4 <- data %>%
  group_by(Abbreviation) %>%
  ggplot(aes(x = reorder(Abbreviation,total_fatalities_rate), 
             y = unemployment_rate)) +
  geom_boxplot() +
  labs(x = "States",  y = "Unemployment Rate") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

b5 <- data %>%
  group_by(Abbreviation) %>%
  ggplot(aes(x = reorder(Abbreviation,total_fatalities_rate), 
             y = blood_alc)) +
  geom_boxplot() +
  labs(x = "States",  y = "Avg. Blood Alcohol Content") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

b6 <- data %>%
  group_by(Abbreviation) %>%
  ggplot(aes(x = reorder(Abbreviation,total_fatalities_rate), 
             y = vehicle_miles_per_capita)) +
  geom_boxplot() +
  labs(x = "States",  y = "Vehicle Miles per Capita") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
        
grid.arrange(b1, b6, b3, b4, b2, b5, ncol=3, nrow=2)
```

> Finally, a boxplot of the distribution across all years is built for a subset
of the variables. This is presented in **Figure 7**. As expected, the state with
the highest average fatality rate is Wyoming, and the state with the smallest is 
Rhode Island. When viewing the vehicle miles per capita, the relationship is striking -
Wyoming is also the state with the highest average vehicle miles driven per capita
across the years surveyed. The population aged 14-24 is also presented in this 
fashion but no major observations can be determined. We will the variables that
we've identified in the EDA as important to build the causal models.

\newpage
# Preliminary Model

### Linear Model 

> A linear model is a sensible starting place because we have non-independence in 
this data set. This dataset also contains some variability, since we have repeated 
measures taken over time, (i.e, finding the fatality rate of each state every year). 
A linear model can help us determine the relationships between different variables 
and the outcome and allow us to better understand the source of variability within 
the dataset.

```{r preliminary model, message=FALSE, warning=FALSE, results='asis'}
lsdv_mod <- plm(total_fatalities_rate ~  minimum_drinking_age + state, 
                index=c("state", "year"), model = "within", data=data)

stargazer(lsdv_mod, type = "latex", header=FALSE,
          omit.stat = c("ser","f","adj.rsq"), dep.var.labels = "",
          column.labels = c("Within"), title="Preliminary Model")
```

### Discussion on Linear Model

> We incorporate dummy variables for the year and for all states. The model doesn't 
include any other variable, so omitted variable bias could be present, however, the 
results imply that minimum drinking age does have an impact on the total fatality 
rate within states.

\newpage
# Expanded Model 

### Modeling

```{r expanded model, message=FALSE, warning=FALSE, results='asis'}
# create speed limit over 70 variable
data <- data %>% 
  mutate(speed_lim_70 = ifelse(speed_limit == 55 | speed_limit == 65, 0, 1))


exp_mod <- plm(
  log(total_fatalities_rate) ~ minimum_drinking_age + state + blood_alc + 
    per_se_laws + primary_seatbelt_law + secondary_seatbelt_law + speed_lim_70 + 
    graduated_drivers_license_law + log(pct_population_14_to_24) + 
    log(unemployment_rate) + log(vehicle_miles_per_capita),
  index=c("state", "year"), 
  model = "within", 
  data=data
)

stargazer(exp_mod, type = "latex", header=FALSE,
          omit.stat = c("ser","f","adj.rsq"), dep.var.labels = "",
          column.labels = c("Within"), title="Expanded Model")

```

> Based on the expanded model, the vehicle miles driven per capita, unemployment rate, 
percent of population between 14 and 24, as well as per se laws and each year of 
observation were statistically significant.  

\newpage
# State-Level Fixed Effects 

### Modeling

```{r state level fixed effects, message=FALSE, warning=FALSE, results='asis'}

within.model <- plm(
  total_fatalities_rate ~ as.numeric(year) + 
    blood_alc +
    per_se_laws + 
    primary_seatbelt_law + 
    speed_limit + 
    graduated_drivers_license_law +               
    log(`pct_population_14_to_24`) + 
    log(unemployment_rate) + 
    log(vehicle_miles_per_capita), 
  data = data, 
  index=c("state", "year"), 
  model="within"
)


pool.model <- plm(
  total_fatalities_rate ~ as.numeric(year) + 
    blood_alc +
    per_se_laws + 
    primary_seatbelt_law + 
    speed_limit + 
    graduated_drivers_license_law +               
    log(`pct_population_14_to_24`) + 
    log(unemployment_rate) + 
    log(vehicle_miles_per_capita), 
  data = data, 
  index=c("state", "year"),
  model="pooling"
)

fd.model <- plm(total_fatalities_rate ~ as.numeric(year) + 
    blood_alc +
    per_se_laws + 
    primary_seatbelt_law + 
    speed_limit + 
    graduated_drivers_license_law +               
    log(`pct_population_14_to_24`) + 
    log(unemployment_rate) + 
    log(vehicle_miles_per_capita), 
  data = data, 
  index=c("state", "year"), 
  model="fd"
)

between.model <- plm(
  total_fatalities_rate ~ as.numeric(year) +  
    blood_alc +
    per_se_laws + 
    primary_seatbelt_law + 
    speed_limit + 
    graduated_drivers_license_law +               
    log(`pct_population_14_to_24`) + 
    log(unemployment_rate) + 
    log(vehicle_miles_per_capita), 
  data = data, 
  index=c("state", "year"),
  model="between"
)

random.model <- plm(
  total_fatalities_rate ~ as.numeric(year) +  
    blood_alc +
    per_se_laws + 
    primary_seatbelt_law + 
    speed_limit + 
    graduated_drivers_license_law +               
    log(`pct_population_14_to_24`) + 
    log(unemployment_rate) + 
    log(vehicle_miles_per_capita), 
  data = data, 
  index=c("state", "year"),
  model="random"
)


stargazer(pool.model, fd.model, between.model, within.model, random.model, 
          type = "latex", header=FALSE,
          omit.stat = c("ser","f","adj.rsq"), dep.var.labels = "",
          column.labels = c("Pooled", "FD", "Between", "Within", "Random"),
          title="Comparison of Models")
```

> State level fixed effect models are estimated using the `plm` function in R, which
takes advantage of the panel data structure for model estimations. A *within*, *pooling*,
*first difference*, *between*, and *random effects* model are estimated and compared
in **Table 1**.

### Model analysis

```{r message=FALSE, warning=FALSE}
pFtest(within.model, pool.model)
```

> A pFtest is conducted to determine where state and time fixed effects should be included. 
Our pFtest returns a significant p value, meaning that we reject the null hypothesis. 
This means we should include the state and time fixed effects in our model.

```{r message=FALSE, warning=FALSE}
pwfdtest(fd.model, data=data, index=c("state", "year"), h0="fe")
```

```{r message=FALSE, warning=FALSE}
pwfdtest(fd.model, data=data, index=c("state", "year"), h0="fd")
```

```{r message=FALSE, warning=FALSE}
phtest(within.model, random.model)
```

### Blood Alcohol Affects

> The fixed effect model estimates that blood alcohol has a coefficient of -6.163. 
In action, this means that for a unit increase in blood alcohol limit, the fatality 
rate will decrease by approximately 6 fatalities per 100,000 people. However in our 
case, blood alcohol is measured in hundreths and tenths rather than in unitary increments, 
so a full unit increase isn't likely to take place. In the Pooled model, the coefficient 
for blood alcohol is -8.736, which is the largest impact out of the five models. 
The coefficient for the First Difference model is -3.092, the smallest impact of 
the models. The between model has a similar coefficient of -3.313, and lastly the 
Random Effects model has a coefficient of -6.610, very simlilar to the Fixed Effect 
Model. All of these coefficients are significant.

### Per se Laws

> The Fixed Effect model estimates that the coefficient for the per-se parameter 
is -1.219. In our data, the per se parameter is a one-hot-encoded variable with 
some values being between zero and one, in cases where states took on the law mid-year. 
In the case where a state does have per se laws in a given year, our model estimates 
that the rate of fatalities will decrease by 1.2 per 100,000 people. The is the 
largest impact of the five models, with the coefficients of the Pooled Effect, First 
Difference, Between, and Random Effect being -0.881, -0.669, -0.467, and -1.178 
respectively. All of these coefficients are significant.

### Primary Seat Belt Laws

> The primary seat belt law variable is also a Boolean, indicating whether a state 
uses primary seat belt laws or secondary. For the Pooled model, the coefficient is 
not significant and is -0.322, a fairly small impact. Similarly, the coefficient for 
the First Difference model is -0.201, and the coefficient for the Between model is -0.491. 
This variable is not significant in either model. However, the coefficients for both 
the Fixed Effect and Random Effect models are both significant, being -0.867 and -0.833 
respectively. 

### Reliability of Estimates

> The Wooldridge first-difference test for serial correlation in panels returns 
significant p values for both the Fixed Effect (within) and First Difference models, 
however the p value for the Fixed Effect model is far more significant. This leads 
us to believe that the Fixed Effect model is more reliable than the First Difference
and Pooled models. Moving on, the outcome of the Hausmen test is not significant, 
meaning we do not reject the null hypothesis that random effects are appropriate, 
suggesting that we should not use the Fixed Effect model. Because of these outcomes, 
the estimates created by the Random Effects model will be the most reliable.

### Model Assumptions 

### Are the assumptions reasonable?

\newpage
# Random Effects Model 

Instead of estimating a fixed effects model, should you have estimated a random 
effects model?

```{r random-effect-model estimate, warning=FALSE, message=FALSE, results='asis'}

data <- data %>%
  mutate(speed_limit = ifelse(sl55 >= 0.5, 55,
                       ifelse(sl65 >= 0.5, 65,
                       ifelse(sl70 >= 0.5, 70,
                       ifelse(sl75 >= 0.5, 75,
                       ifelse(slnone >= 0.5, 'none', 0)
                       ))))) 

data = data %>% mutate(
    seatbelt = factor(seatbelt), # 'seatbelt' categorizes primary or secondary
    speed_limit_70plus = ifelse(speed_limit == 55 | speed_limit == 65, 0, 1)
  )

data <- data %>% mutate(blood_alcohol_limit_10 = ifelse(bac10 >= 0.5, 1, 0),
                        blood_alcohol_limit_08 = ifelse(bac08 >= 0.5, 1, 0)
                       ) %>% 
                mutate(bac = ifelse(blood_alcohol_limit_10==1, '10', 
                             ifelse(blood_alcohol_limit_08==1, '8', 'none')))

random.effect.model <- plm(log(total_fatalities_rate) ~ 
                            year_of_observation + 
                            factor(bac) +
                            per_se_laws + 
                            primary_seatbelt_law + 
                            secondary_seatbelt_law + 
                            speed_limit_70plus + 
                            graduated_drivers_license_law + 
                            pct_population_14_to_24 +
                            unemployment_rate +
                            vehicle_miles_per_capita,
                            #log(pct_population_14_to_24) + 
                            #log(unemployment_rate) + 
                            #log(vehicle_miles_per_capita), 
                           data = data,
                           index = c("state", "year"),
                           model = "random")

stargazer(random.effect.model, type='latex', header=FALSE,
          omit.stat = c("ser","f","adj.rsq"), 
          dep.var.labels = "", title='Random Effects Model')

```

```{r cftest, message=FALSE, warning=FALSE}
coeftest(random.effect.model, vcov. = vcovHC, type = "HC1")
```

### Assumptions of Random Effects

> The first assumption of the random effect model is that there are no perfect 
linear relationships among the explanatory variables. 

```{r vif for RE model, message=FALSE, warning=FALSE}
library(car)
car::vif(random.effect.model)
```
**We see high values for percent_pop_aged_14_to_24, vehicle_miles_per_capita 
indicating the possible presence of multicollinearity in these variables.**

> The second assumption is that there is no correlation between the unobserved 
random and fixed effects and the explanatory variables. Using a random effects 
model imposes the error structure that the error term** $v_{it}$ **is 
equal to the sum of variation between groups and variation within groups onto the 
model residuals, allowing to properly specify the residuals and more efficiently 
estimate the coefficients of interest. This requires the assumption of independence 
between random effects and the other predictors in the model. The assumptions for 
the fixed effect model are discussed above, the additional assumption of independence 
of random effects and other predictors in the model is evaluated below. The test 
we run is the Hausman Test for fixed versus random effects. The null hypothesis 
is that the random effects model is acceptable while the alternative hypothesis 
is that there is correlation between residuals and predictors, meaning that we 
should use the FE model.**

We conduct a Hausman test for random vs. fixed effects using `phtest`. We perform 
this test with an $\alpha = 0.05$

```{r, model comparison between within and random efect moidels, message=FALSE, warning=FALSE}
res <- phtest(within.model, random.effect.model)
# res
```
With a p-value of `r res$p.value` less than $\alpha$, we reject the null 
hypothesis that random effects are appropriate, suggesting that we should use the 
fixed models. The random effects model is not likely to be consistent in this case.

>The third assumption is that of homoskedastic errors, which we can test below 
using the Breusch-Pagan Lagrange Multiplier for random effects. Null is no 
panel effect: 

```{r pcdtest for Random Effect Model}
plmtest(random.effect.model)
```

> Here we failed to reject the null and conclude that random effects is 
not appropriate.

### Note on Assumptions

> As we have seen that the assumptions for random effect model are not met. If 
we were to inappropriately estimate a random effect model, we would be incorrectly 
assuming that the random effects and other predictors are independent of one another. 
This would lead to omitted variable bias as the correlation between the random 
effects and the explanatory variables of interest would not allow for accurate 
estimation of the coefficient. Standard errors will also be biased as we are 
assuming that the random effects, which are included in the error term, are 
incorrectly uncorrelated with the predictors - given that there is correlation, 
this will introduce bias into the standard errors.

\newpage
# Model Forecasts 

### Data on Vehicle Miles Traveled

> We have downloaded population data from https://fred.stlouisfed.org/series/POPTHM
and vehicle driven data from https://fred.stlouisfed.org/series/TRFVOLUSM227NFWA.
Population includes resident population plus armed forces overseas. The monthly 
estimate is the average of estimates for the first of the month and the first of 
the following month. Vehicle Miles Traveled and the 12-Month Moving Vehicle Miles 
Traveled series are created by appending the recent monthly figures from the 
FHWA’s Traffic Volume Trends to their Historic Monthly Vehicle Miles Traveled 
(VMT) data file. We have defined the pandemic period between March 2020 through 
March 2021 when the Covid vaccine became widely available.

```{r, message=FALSE, warning=FALSE, fig.height=6, fig.width=8, fig.cap="Vehicle Miles Traveled Series from the St. Louis Fed"}
library(fredr)

fredr_set_key("cd565a10e83d56f9f1150d5a2c067e2a")

data.vhcl <- fredr(
                   series_id = "TRFVOLUSM227NFWA",
                   observation_start = as.Date("2018-01-01"),
                   observation_end = as.Date("2023-08-01")
                  ) %>% dplyr::select(date,value) %>% as_tsibble(index = date)
data.pop <- fredr(
                  series_id = "POPTHM",
                  observation_start = as.Date("2018-01-01"),
                  observation_end = as.Date("2023-08-01")
                 )%>% dplyr::select(date, value) %>% as_tsibble(index = date)

# Merge vehicle miles driven and population data
data.temp <- merge(x = data.vhcl, y = data.pop, by = "date")

# Calculate vehicle miles per capita
data.temp <- data.temp %>% 
  mutate(vehicle_miles_per_capita = 1000 * value.x / value.y)

data.vhcl.ml.per.capita <- data.temp[, c('date', 'vehicle_miles_per_capita')]
data.vhcl.ml.per.capita$year = year(data.vhcl.ml.per.capita$date)
data.vhcl.ml.per.capita$month = month(data.vhcl.ml.per.capita$date)

data.vhcl.ml.per.capita <- data.vhcl.ml.per.capita %>%
  mutate(group = ifelse(year < 2020, "pre-pandemic",
               ifelse(year == 2020 | year == 2021, "pandemic", 
                      "post-pandemic")))


data.pre.pandemic <- data.vhcl.ml.per.capita %>% 
  filter(year == 2018)
data.pandemic <- data.vhcl.ml.per.capita %>% 
  filter(year == 2020 | year == 2021)
data.pandemic.arranged <- data.pandemic %>% 
  arrange(month)

vehicle_miles_per_capita.diff <- 
  data.pre.pandemic$vehicle_miles_per_capita - 
  data.pandemic.arranged$vehicle_miles_per_capita

drive_pandemic <- data.pandemic %>% slice(3:15)
data.pandemic$group <- 'pandemic'
data.pre.pandemic$group <- 'pre_pandemic'
data.pandemic.pre.post.comparison <- rbind(data.pre.pandemic, data.pandemic)

plot.orig <- data.vhcl.ml.per.capita %>% 
  ggplot(aes(x = date, y = vehicle_miles_per_capita, color=group)) + 
  geom_line() + xlab("Date") + ylab("Veh. Miles per Capita") +
  ggtitle('Miles Driven during Pandemic')

plot.comparison <- ggplot(
  data.pandemic.pre.post.comparison, 
         aes(x = month, 
             y = vehicle_miles_per_capita, 
             group = group)) + 
  geom_point(aes(color=group)) + 
  geom_smooth(aes(color=group)) + 
  xlab('Month') +
  ylab('Miles Driven') + 
  ggtitle('Pre/Post Pandemic Miles Driven')

(plot.orig / plot.comparison)
```

### Forecasting changes in driving

```{r assess decrease, message=FALSE, warning=FALSE}

max_prepandemic <- max(data.pre.pandemic$vehicle_miles_per_capita)
min_pandemic <- min(data.pandemic$vehicle_miles_per_capita)

```

> The pandemic caused a rapid decrease in the vehicle miles traveled per capita.
To forecast the impact of this decrease, we assessed the pre-pandemic peak to the
pandemic lull - which was a decrease of around `r round(max_prepandemic-min_pandemic,1)`
miles per capita.
  


\newpage
# Evaluate Error 

### Consequences of Serial Correlation / Heteroskadicity

> According to literature, the consequences of serial correlation and heteroskedacity 
in panel data models is a loss in efficiency of the estimates (Jianhong). This means
that the true coefficients relative to the estimated coefficients likely
have higher variance than what is currently estimated in the standard errors.

### Are there serial correlations or heteroskedasticity

> Yes, while testing the random effects model, we conducted the Lagrange Multiplier
test and rejected the null hypothesis. We also a Hausman test and rejected the null
hypothesis. By rejecting the null hypothesis in both cases, we can conclude that
there exists both serial correlation and heteroskedasticity.

\newpage
# References

(1) Jianhong Wu, A joint test for serial correlation and heteroscedasticity in fixed-T panel regression 
models with interactive effects, Economics Letters, Volume 197, 2020, 109594, ISSN 0165-1765, 
https://doi.org/10.1016/j.econlet.2020.109594. (https://www.sciencedirect.com/science/article/pii/S0165176520303578)
